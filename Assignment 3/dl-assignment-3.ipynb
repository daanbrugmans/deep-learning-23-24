{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third assignment for the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "\n",
    "**Group:**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Experiment with convolutional neural networks\n",
    "2. Train a convolutional neural network on a speech dataset\n",
    "3. Investigate the effect of dropout and batch normalization\n",
    "4. Define and train a residual neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "If you haven't done so already, you will need to install the following additional libraries:\n",
    "* `torch` and `torchvision` for PyTorch,\n",
    "* `d2l`, the library that comes with [Dive into deep learning](https://d2l.ai) book,\n",
    "* `python_speech_features` to compute MFCC features.\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Fix the seed, so outputs are exactly reproducible\n",
    "torch.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Convolution and receptive fields (9 points)\n",
    "\n",
    "We will first define some helper functions to plot the receptive field of a node in a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, title=None, new_figure=True):\n",
    "    if new_figure:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "    im = plt.imshow(img, interpolation='none', aspect='equal', cmap='gray')\n",
    "    ax = plt.gca();\n",
    "\n",
    "    # plot pixel numbers and grid lines\n",
    "    ax.set_xticks(np.arange(0, img.shape[1], 1))\n",
    "    ax.set_yticks(np.arange(0, img.shape[0], 1))\n",
    "    ax.set_xticklabels(np.arange(0, img.shape[1], 1))\n",
    "    ax.set_yticklabels(np.arange(0, img.shape[0], 1))\n",
    "    ax.set_xticks(np.arange(-.5, img.shape[1], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, img.shape[0], 1), minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=1.5)\n",
    "\n",
    "    # hide axis outline\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "# set all weights in the network to one,\n",
    "# all biases to zero\n",
    "def fill_weights_with_ones(network):\n",
    "    for name, param in network.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            param.data = torch.ones_like(param.data)\n",
    "        elif 'bias' in name:\n",
    "            param.data = torch.zeros_like(param.data)\n",
    "    return network\n",
    "\n",
    "def compute_receptive_field(network, input_size=(15, 15), binary=True):\n",
    "    assert isinstance(network, torch.nn.Sequential), 'This only works with torch.nn.Sequential networks.'\n",
    "    for layer in network:\n",
    "        if not isinstance(layer, (torch.nn.Conv2d, torch.nn.AvgPool2d)):\n",
    "            raise Exception('Sorry, this visualisation only works for Conv2d and AvgPool2d.')\n",
    "\n",
    "    # initialize weights to ones, biases to zeros\n",
    "    fill_weights_with_ones(network)\n",
    "\n",
    "    # find the number of input and output channels\n",
    "    input_channels = None\n",
    "    output_channels = None\n",
    "    for layer in network:\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            if input_channels is None:\n",
    "                # first convolution layer\n",
    "                input_channels = layer.in_channels\n",
    "            output_channels = layer.out_channels\n",
    "    if input_channels is None:\n",
    "        input_channels = 1\n",
    "\n",
    "    # first, we run the forward pass to compute the output shape give the input\n",
    "\n",
    "    # PyTorch expects input shape [samples, channels, rows, columns]\n",
    "    x = torch.zeros(1, input_channels, *input_size)\n",
    "    x.requires_grad = True\n",
    "\n",
    "    # forward pass: apply each layer in the network\n",
    "    y = x\n",
    "    y.retain_grad()\n",
    "    ys = [y]\n",
    "    for layer in network:\n",
    "        y = layer(y)\n",
    "        # keep track of the intermediate values so we can plot them later\n",
    "        y.retain_grad()\n",
    "        ys.append(y)\n",
    "\n",
    "    # second, we run the backward pass to compute the receptive field\n",
    "\n",
    "    # create gradient input: zeros everywhere, except for a single pixel\n",
    "    y_grad = torch.zeros_like(y)\n",
    "    # put a one somewhere in the middle of the output\n",
    "    y_grad[0, 0, (y_grad.shape[2] - 1) // 2, (y_grad.shape[3] - 1) // 2] = 1\n",
    "\n",
    "    # compute the gradients given this single one\n",
    "    y.backward(y_grad)\n",
    "\n",
    "    # receptive field is now in the gradient at each layer\n",
    "    receptive_fields = []\n",
    "    for y in ys:\n",
    "        # the gradient for this layer shows us the receptive field\n",
    "        receptive_field = y.grad\n",
    "        if binary:\n",
    "            receptive_field = receptive_field > 0\n",
    "        receptive_fields.append(receptive_field)\n",
    "    return receptive_fields\n",
    "\n",
    "def plot_receptive_field(network, input_size=(15, 15), binary=True):\n",
    "    receptive_fields = compute_receptive_field(network, input_size, binary)\n",
    "    \n",
    "    # plot the gradient at each layer\n",
    "    plt.figure(figsize=(4 * len(receptive_fields), 4))\n",
    "    for idx, receptive_field in enumerate(receptive_fields):\n",
    "        plt.subplot(1, len(receptive_fields), idx + 1)\n",
    "        # the last element of ys contains the output of the network\n",
    "        if idx == len(receptive_fields) - 1:\n",
    "            plot_title = 'output (%dx%d)' % (receptive_field.shape[2], receptive_field.shape[3])\n",
    "        else:\n",
    "            plot_title = 'layer %d input (%dx%d)' % (idx, receptive_field.shape[2], receptive_field.shape[3])\n",
    "        # plot the image with the receptive field (sample 0, channel 0)\n",
    "        show_image(receptive_field[0, 0], new_figure=False, title=plot_title)\n",
    "        if not binary:\n",
    "            plt.colorbar(fraction=0.047 * receptive_field.shape[0] / receptive_field.shape[1])\n",
    "\n",
    "def receptive_field_size(network, input_size=(15, 15), binary=True):\n",
    "    receptive_fields = compute_receptive_field(network, input_size, binary)\n",
    "    return torch.count_nonzero(torch.flatten(receptive_fields[0][0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions, we can define a network and plot the receptive field of a pixel in the output.\n",
    "\n",
    "**(a) Run the code to define a network with one 3\u00d73 convolution layer and plot the images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 1, kernel_size=(3, 3)),\n",
    ")\n",
    "plot_receptive_field(net, input_size=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read these images as follows:\n",
    "* On the left, you see the input size of the network (here: 15 x 15 pixels) and the receptive field for one pixel in the output.\n",
    "* On the right, you see the output size of the network (here: 13 x 13 pixels).\n",
    "\n",
    "To visualize the receptive field of this network, we used the following procedure:\n",
    "* We selected one pixel of the output (shown as the white pixel in the center in the image on the right).\n",
    "* We computed the gradient for this pixel and plotted the gradient with respect to the input (the image on the left).\n",
    "* This shows you the receptive field of the network: the output for the pixel we selected depends on these 9 pixels in the input.\n",
    "\n",
    "**(b) Use this method to plot the receptive field of a pixel in the output of a convolution layer with a kernel size of 5\u00d75.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot the receptive field of a 5x5 convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the result, you will see that two things have changed: the receptive field and the output size.\n",
    "\n",
    "**(c) How do the receptive field size and the output size depend on the kernel size? Give a formula.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of parameters\n",
    "\n",
    "In the previous question, you saw how the receptive fields of a 3x3 convolution differs from a 5x5 kernel convolution. But this is not the only difference: there is also a difference in the number of parameters in the network.\n",
    "\n",
    "We can count the number of parameters in the network by computing the number of elements (e.g., the weights and biases in a convolution kernel) in the parameter list of the PyTorch network.\n",
    "\n",
    "We'll define a small helper function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameter_count(network):\n",
    "    # sum the number of elements in each parameter of the network\n",
    "    count = sum([param.data.numel() for param in network.parameters()])\n",
    "    print('%d parameters' % count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Use the function to count the number of parameters for a 3x3 convolution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 1, kernel_size=(3, 3)),\n",
    ")\n",
    "plot_receptive_field(net, input_size=(15, 15))\n",
    "print_parameter_count(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Do the same to count the number of parameters for a 5x5 convolution.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO count the number of parameters of a 5x5 convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Explain the results by showing how to _compute_ the number of parameters for the 3x3 and 5x5 convolutions.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these computations we used convolution layers with one input and one output channel.\n",
    "\n",
    "We can also compute the results for a layer with a different number of channels.\n",
    "\n",
    "**(g) Define a network with a 5x5 convolution, 2 input channels and 3 output channels. Print the number of parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO count the number of parameters of a 5x5 convolution with 2 input channels and 3 output channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Show how to compute the number of parameters for this case.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving the size of the input image\n",
    "\n",
    "The PyTorch documentation for [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) describes the parameters that you can use to define a convolutional layer. We will explore some of those parameters in the next questions.\n",
    "\n",
    "In the previous plot, you may have noticed that the output (13x13 pixels) was slightly smaller than the input (15x15 pixels).\n",
    "\n",
    "**(i) Define a network with a single 3x3 convolutional layer that produces an output that has the same size as the input.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Use 1 input and 1 output channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define a network with a 3\u00d73 kernel size that takes a 15\u00d715 input image\n",
    "#      and produces a 15\u00d715 output image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(j) Define a network with a single 5x5 convolutional layer that preserves the input size.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Define a network with a 5\u00d75 kernel size that takes a 15\u00d715 input image\n",
    "#      and produces a 15\u00d715 output image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with some other values to see how this parameter behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple layers\n",
    "\n",
    "As you have just seen, one way to increase the size of the receptive field is to use a larger convolution kernel. But another way is to use more than one convolution layer.\n",
    "\n",
    "**(k) Define a network with two 3x3 convolutions, preserving the image size. Show the receptive field and the number of parameters.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "For this visualisation, do not use any activation functions, and use 1 channel everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define a network with two 3x3 convolutions\n",
    "net = torch.nn.Sequential()\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(15, 15))\n",
    "print_parameter_count(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have two layers, the visualization shows an extra image. From right to left, we have:\n",
    "* Right: the output size and a single active pixel.\n",
    "* Middle: the receptive field for the single output pixel between the first and second convolution.\n",
    "* Left: the receptive field for the single output pixel in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now tried two ways to increase the receptive field size: increasing the kernel size, and using multiple layers.\n",
    "\n",
    "**(l) Compare the number of parameters required by the two options. Which one is more parameter-efficient?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Variations on convolution (8 points)\n",
    "\n",
    "### Pooling\n",
    "\n",
    "We can also increase the size of the receptive field by using a pooling layer.\n",
    "\n",
    "**(a) Construct a network with a 3x3 convolution (preserving the input size) followed by a 2x2 average pooling. Plot the receptive field and print the number of parameters.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Use 1 input and 1 output channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define a network with two 3x3 convolutions\n",
    "net = torch.nn.Sequential()\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(14, 14))\n",
    "print_parameter_count(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Explain the number of parameters in this convolution + pooling network.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n",
    "\n",
    "A third option to increase the receptive field is _dilation_.\n",
    "\n",
    "**(c) Define a network with 3x3 convolution with dilation that preserves the input size.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define a network with one 3x3 convolution with dilation\n",
    "net = torch.nn.Sequential()\n",
    "# the output should also be 15x15 pixels\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(15, 15))\n",
    "print_parameter_count(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Explain how dilation affects the receptive field.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using strides\n",
    "\n",
    "By default, convolution layers use a stride of 1.\n",
    "\n",
    "**(e) Change the network to use a stride of 2 and plot the result.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO increase the stride to 2\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 1, kernel_size=(3, 3), padding=(1, 1)),\n",
    ")\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(14, 14))\n",
    "print_parameter_count(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Explain the new output size and compare the result with that of pooling.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Explain how the stride affects the receptive field of this single convolution layer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Explain the number of parameters for this network.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Combining layers (7 points)\n",
    "\n",
    "As you have seen, there are multiple ways to increase the receptive field. You can make interesting combinations by stacking multiple layers.\n",
    "\n",
    "Let's try a few ways to make networks with a large receptive field. For each of the questions in this section:\n",
    "\n",
    "* Create a network where a pixel in the output has a 9x9 receptive field.\n",
    "* Use 3 input channels and 3 output channels in every layer.\n",
    "* In convolution layers, try to preserve the input size as much as possible.\n",
    "\n",
    "**(a) Make a network with a single convolution that satisfies the above conditions.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "net = ...\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(14, 14))\n",
    "print_parameter_count(net)\n",
    "assert receptive_field_size(net) == 9*9, \"Receptive field of output pixel should be a 9x9 square\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many popular network architectures use a sequence of 3x3 convolutions.\n",
    "\n",
    "**(b) Use only 3x3 convolutions.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "net = ...\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(14, 14))\n",
    "print_parameter_count(net)\n",
    "assert receptive_field_size(net) == 9*9, \"Receptive field of output pixel should be a 9x9 square\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Use a 2x2 average pooling layer in combination with one or more 3x3 convolutions.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "net = ...\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(14, 14))\n",
    "print_parameter_count(net)\n",
    "assert receptive_field_size(net) == 9*9, \"Receptive field of output pixel should be a 9x9 square\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Copy the previous convolution + pooling network and replace the pooling layer with a strided convolution layer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "net = ...\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(14, 14))\n",
    "print_parameter_count(net)\n",
    "assert receptive_field_size(net) == 9*9, \"Receptive field of output pixel should be a 9x9 square\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Construct a network with exactly two 3x3 convolutions. Use dilation to get a receptive field of 9x9 pixels.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "net = ...\n",
    "print(net)\n",
    "plot_receptive_field(net, input_size=(14, 14))\n",
    "print_parameter_count(net)\n",
    "assert receptive_field_size(net) == 9*9, \"Receptive field of output pixel should be a 9x9 square\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) For each of the methods, list the number of layers, the number of parameters, and the size of the output of the network:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method                    | Layers | Parameters | Output size |\n",
    "|---------------------------|--------|------------|-------------|\n",
    "| One 9x9 convolution       |    1   |     732    |    14x14    | \n",
    "| Many 3x3 convolutions     |        |            |             |\n",
    "| With pooling              |        |            |             |\n",
    "| With strided convolution  |        |            |             |\n",
    "| With dilation             |        |            |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Compare the methods in terms of the number of parameters.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Compare the methods in terms of the output size. How much downsampling do they do?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Padding in very deep networks (2 points)\n",
    "\n",
    "Without padding, the output of a convolution is smaller than the input. This limits the depth of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) How often can you apply a 3x3 convolution to a 15x15 input image?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum number of layers\n",
    "number_of_times = 25\n",
    "\n",
    "# create a 15x15 input\n",
    "x = torch.zeros(1, 1, 15, 15)\n",
    "print('input size: %dx%d' % (x.shape[2], x.shape[3]))\n",
    "\n",
    "# create a 3x3 convolution\n",
    "conv = torch.nn.Conv2d(1, 1, kernel_size=(3, 3))\n",
    "\n",
    "for n in range(number_of_times):\n",
    "    # apply another convolution\n",
    "    x = conv(x)\n",
    "    print('layer %d, output size: %dx%d' % (n + 1, x.shape[2], x.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in this assignment, you have used padding to address this problem. This seems ideal.\n",
    "\n",
    "**(b) Copy the previous code, add some padding, and show that we can now have an infinite number of layers.**\n",
    "\n",
    "(We are computer scientists and not mathematicians, so for the purpose of this question we'll consider 'infinite' to be equal to 25.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Does it really work like this? Have a look at the following experiment.**\n",
    "\n",
    "* We simulate a convolution network with 25 convolution layers, with 3x3 kernels and the right amount of padding.\n",
    "* We set the weights to 1/9 (so that the sum of the 3x3 kernel is equal to 1) and set the bias to zero.\n",
    "* We give this network a 15x15-pixel input filled with ones.\n",
    "* We plot the output of layers 5, 10, 15, 20, and 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 15x15 input filled with ones\n",
    "x = torch.ones(1, 1, 15, 15)\n",
    "\n",
    "# create a 3x3 convolution\n",
    "conv = torch.nn.Conv2d(1, 1, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "# set weights to 1/9 (= sum to one), bias to zero\n",
    "conv.weight.data = torch.ones_like(conv.weight.data) / 9\n",
    "conv.bias.data = torch.zeros_like(conv.bias.data)\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for n in range(1, 26):\n",
    "    # apply another convolution\n",
    "    x = conv(x)\n",
    "    # print('layer %d, output size: %dx%d' % (n + 1, x.shape[2], x.shape[3]))\n",
    "    if n % 5 == 0:\n",
    "        plt.subplot(1, 5, n // 5)\n",
    "        plt.imshow(x[0, 0].detach().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('layer %d' % n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Explain the pattern that we see in the output of the final layers. How does this happen, and what does this mean for our very deep networks?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Spoken digits dataset (4 points)\n",
    "\n",
    "Time for some practical experiments. The d2l book uses a dataset of images as a running example (FashionMNIST). In this assignment we will investigate CNNs in a completely different domain: speech recognition.\n",
    "\n",
    "The dataset we use is the free spoken digits dataset, which can be found on https://github.com/Jakobovski/free-spoken-digit-dataset. This dataset consists of the digits 0 to 9, spoken by different speakers. The data comes as .wav files.\n",
    "\n",
    "**(a) Use the commands below (or a similar tool) to download the dataset. You can also use `git clone` to clone the repository mentioned above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! mkdir -p free-spoken-digit-dataset\n",
    "#! wget -O - https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/heads/master.tar.gz | tar xzv -C free-spoken-digit-dataset --strip-components=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function to load the data. We pad/truncate each sample to the same length.\n",
    "The raw audio is usually stored in 16 bit integers, with a range -32768 to 32767, where 0 represents no signal. Before using the data, it should be normalized. A common approach is to make sure that the data is between 0 and 1, between -1 and 1, or zero-mean unit-variance.  Not all of these work well on this data, so later on, if your\n",
    "network doesn't seem to learn anything: try a different method to see if that works better.\n",
    "\n",
    "**(b) Update the below code to normalize the data to a reasonable range.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate = 8000\n",
    "def load_waveform(file, size = 6000):\n",
    "    samplerate, waveform = wavfile.read(file)\n",
    "    # Take first 6000 samples from waveform. With a samplerate of 8000 that corresponds to 3/4 second\n",
    "    # Pad with 0s if the file is shorter\n",
    "    waveform = np.pad(waveform,(0,size))[0:size]\n",
    "    # Normalize waveform\n",
    "    # TODO: Your code here.\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads all .wav files in a directory, and makes it available in a pytorch dataset.\n",
    "\n",
    "**(c) Load the data into a variable `data`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpokenDigits(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        digits_x = []\n",
    "        digits_y = []\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".wav\"):\n",
    "                waveform = load_waveform(os.path.join(data_dir, file))\n",
    "                label = int(file[0])\n",
    "                digits_x.append(waveform)\n",
    "                digits_y.append(label)\n",
    "        # convert to torch tensors\n",
    "        self.x = torch.from_numpy(np.array(digits_x, dtype=np.float32))\n",
    "        # add an extra dimension to represent the \"channels\" (we start with 1 channel of data)\n",
    "        self.x = self.x.unsqueeze(1)\n",
    "        self.y = torch.from_numpy(np.array(digits_y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Describe the dataset: how many samples are there, how many features does each sample have? How many classes are there?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code to play samples from the dataset to give you an idea what it \"looks\" like.\n",
    "\n",
    "Note: If this step doesn't work in your notebook, then you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "def play(sample):\n",
    "    print(f'Label: {sample[1]}')\n",
    "    return Audio(sample[0][0].numpy(), rate=samplerate)\n",
    "play(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, we split the dataset into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop = 2/3\n",
    "train_count = int(len(data) * train_prop)\n",
    "train, test = torch.utils.data.random_split(data, [train_count, len(data) - train_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above uses 2/3 of the data for training.\n",
    "\n",
    "**(e) Discuss an advantage and disadvantage of using more of the data for training.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split the data into batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {'batch_size': 32}\n",
    "train_iter = torch.utils.data.DataLoader(train, **data_params)\n",
    "test_iter  = torch.utils.data.DataLoader(test,  **data_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 One-dimensional convolutional neural network (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a network architecture. We will use a combination of convolutional layers and pooling.\n",
    "Note that we use 1d convolution and pooling here, instead of the 2d operations used for images.\n",
    "\n",
    "**(a) Complete the network architecture, look at the d2l book [chapter 7](http://d2l.ai/chapter_convolutional-neural-networks/index.html) and [chapter 8](http://d2l.ai/chapter_convolutional-modern/index.html) for examples.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net():\n",
    "    return torch.nn.Sequential(\n",
    "        nn.Conv1d(1, 4, kernel_size=5), nn.ReLU(),\n",
    "        nn.AvgPool1d(kernel_size=2, stride=2),\n",
    "        # TODO: Add three more convolutional layers, ReLU layers and pooling layers;\n",
    "        #       doubling the number of channels each time\n",
    "        # TODO: Your code here.\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(11872, 128), nn.ReLU(),\n",
    "        nn.Linear(128, 64), nn.ReLU(),\n",
    "        nn.Linear(64, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) The first fully connected layer has input dimension 11872, where does that number come from?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here.\n",
    "\n",
    "Hint: think about how (valid) convolutional layers and pooling layers with stride affect the size of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How many parameters are there in the model? I.e. the total number of weights and biases.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the number of parameters\n",
    "# Hint: use net.parameters() and param.nelement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Suppose that instead of using convolutions, we had used only fully connected layers, while keeping the number of features on each hidden layer the same. How many parameters would be needed in that case approximately?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FashionMNIST dataset used in the book has 60000 training examples. How large is our training set? How would the difference affect the number of epochs that we need? Compare to [chapter 7.6](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html) and [chapter 8.1](http://d2l.ai/chapter_convolutional-modern/alexnet.html) of the book.\n",
    "\n",
    "**(e) How many epochs do you think are needed?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 10 # TODO: change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the code from (a previous edition of) the d2l book to train the network.\n",
    "In particular, the `train` function, defined in [chapter 7.6](http://d2l.ai/chapter_convolutional-neural-networks/lenet.html#training). This function is reproduced below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, num_epochs, lr, device = d2l.try_gpu()):\n",
    "    \"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device, torch.long)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Now train the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(build_net(), train_iter, test_iter, num_epochs=75, lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Did the training converge?<span style=\"float:right\"> (2 point)</span>**\n",
    "\n",
    "**If the training has not converged, maybe you need to change the number of epochs and/or the learning rate.**\n",
    "\n",
    "Hint: This is a non-trivial problem, so your network might take some time to\n",
    "learn. Don't give up too quickly, it might take 50-100 epochs before you\n",
    "see any significant changes in the loss curves.\n",
    "\n",
    "TODO: Document the runs that you have performed and thir results in the table below.\n",
    "\n",
    "| Experiment                | epochs | lr     | train accuracy | test accuracy | converged? |\n",
    "|---------------------------|--------|--------|----------------|---------------|------------|\n",
    "| experiment 1              | 1234   | 1234   |                |               |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Questions and evaluation (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Does the network look like it is overfitting or underfitting? Explain how see this.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Is what we have here a good classifier? Could it be used in a realistic application? Motivate your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: discuss your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Do you think there is enough training data compared to the dimensions of the data and the number of parameters? Motivate your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) How could the classifier be improved? Give at least 2 suggestions.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) The free spoken digits datasets has recordings from several different speakers. Is the test set accuracy a good measure of how well the trained network would perform for recognizing digits spoken by a new, unknown speaker? And if not, how could that be tested instead?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Variations (8 points)\n",
    "\n",
    "One way in which the training might be improved is with dropout or with batch normalization.\n",
    "\n",
    "**(a) Make a copy of the network architecture from 3.6a below, and add dropout.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: see [chapter 8.1](http://d2l.ai/chapter_convolutional-modern/alexnet.html#architecture) for an example that uses dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net_dropout():\n",
    "    return ...  # TODO: your network here\n",
    "\n",
    "train(build_net_dropout(), train_iter, test_iter, num_epochs=200, lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) How does dropout change the results? Does this match what you saw on the simple network last week?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Make a copy of the original network architecture, and add batch normalization to all convolutional and linear layers.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: see [chapter 8.5](http://d2l.ai/chapter_convolutional-modern/batch-norm.html#concise-implementation) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net_batchnorm():\n",
    "    return ...  # TODO: your network here\n",
    "\n",
    "train(build_net_batchnorm(), train_iter, test_iter, num_epochs=15, lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) How does batch normalization change the results? Does this match what you saw on the simple network last week?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual network\n",
    "\n",
    "We can also try to use a residual network. The book has code for a 2d resnet in [Chapter 8.6](http://d2l.ai/chapter_convolutional-modern/resnet.html).\n",
    "\n",
    "**(e) Copy the `Residual` module here, and adapt it for 1d convolutions. Use a kernel size of 5 for the convolution layers.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "Use residual blocks each containing two convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Residual class here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Make a copy of the network architecture from 3.6a, and replace the convolutions with residual blocks.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet():\n",
    "    return ...  # TODO: your network here\n",
    "\n",
    "# TODO train(resnet, train_iter, test_iter, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) How do residual connections change the results?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Feature extraction (5 points)\n",
    "\n",
    "Given enough training data a deep neural network can learn to extract features from raw data like audio and images. However, in some cases it is still necessary to do manual feature extraction, in particular when working with smaller datasets like this one. For speech recognition, a popular class of features are [MFCCs](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum).\n",
    "\n",
    "Here is code to extract these features. You will need to install the `python_speech_features` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "\n",
    "def load_waveform_mfcc(file, size = 6000):\n",
    "    samplerate, waveform = wavfile.read(file)\n",
    "    waveform = np.pad(waveform,(0,size))[0:size] / 32768\n",
    "    return np.transpose(mfcc(waveform, samplerate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Implement a variation of the dataset that uses these features.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpokenDigitsMFCC(torch.utils.data.Dataset):\n",
    "    # TODO: Your code here.\n",
    "    ...\n",
    "\n",
    "data_mfcc = SpokenDigitsMFCC(data_dir) # TODO: your data directory here\n",
    "train_count_mfcc = int(len(data_mfcc) * train_prop)\n",
    "train_mfcc, test_mfcc = torch.utils.data.random_split(data_mfcc, [train_count_mfcc, len(data_mfcc)-train_count_mfcc])\n",
    "train_iter_mfcc = torch.utils.data.DataLoader(train_mfcc, **data_params)\n",
    "test_iter_mfcc  = torch.utils.data.DataLoader(test_mfcc,  **data_params)\n",
    "\n",
    "assert next(iter(train_iter_mfcc))[0].shape == torch.Size([data_params['batch_size'],13,74]), \"There is something wrong with the SpokenDigitsMFCC dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MFCC features will have 13 channels instead of 1 (the `unsqueeze` operation is not needed). \n",
    "\n",
    "**(b) Inspect the shape of the data, and define a new network architecture that accepts data with this shape.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net_mfcc():\n",
    "    # TODO: Your code here.\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Train the network with the MFCC features.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) What would be needed to get a fully neural network approach to work as well as MFCC features?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 57 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 6717cb8 / 2023-09-15</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}