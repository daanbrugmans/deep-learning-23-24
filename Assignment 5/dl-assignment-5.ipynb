{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth assignment for the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "Daan Brugmans, Maximilian Pohl\n",
    "\n",
    "**Group:**\n",
    "31\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Construct a PyTorch `DataSet`\n",
    "1. Train and modify a transformer network\n",
    "1. Experiment with a translation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "If you haven't done so already, you will need to install the following additional libraries:\n",
    "* `torch` for PyTorch,\n",
    "* `d2l`, the library that comes with the [Dive into deep learning](https://d2l.ai) book.  \n",
    "  Note: if you get errors, make sure the right version of the d2l library is installed:\n",
    "  `pip install d2l==1.0.0a1.post0`\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:45.766918704Z",
     "start_time": "2023-10-06T09:28:45.291503286Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "from random import Random\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (IterableDataset, DataLoader)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = d2l.try_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Learning to calculate (5 points)\n",
    "\n",
    "In this assignment we are going to train a neural network to do mathematics.\n",
    "When communicating between humans, mathematics is expressed with words and formulas.\n",
    "The simplest of these are formulas with a numeric answer. For example, we might ask what is `100+50`, to which the answer is `150`.\n",
    "\n",
    "To teach a computer how to do this task, we are going to need a dataset.\n",
    "\n",
    "Below is a function that generates a random formula. Study it, and see if you understand its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:45.795350799Z",
     "start_time": "2023-10-06T09:28:45.298070126Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_integer(length: int, signed: bool = True, rng: Random = Random()):\n",
    "    max = math.pow(10, length)\n",
    "    min = -max if signed else 0\n",
    "    return rng.randint(min, max)\n",
    "\n",
    "\n",
    "def random_formula(complexity: int, signed: bool = True, rng: Random = Random()):\n",
    "    \"\"\"\n",
    "    Generate a random formula of the form \"a+b\" or \"a-b\".\n",
    "    complexity is the maximum number of digits in the numbers.\n",
    "    \"\"\"\n",
    "    a = random_integer(complexity, signed, rng)\n",
    "    b = random_integer(complexity, False, rng)\n",
    "    is_addition = not signed or rng.choice([False, True])\n",
    "    if is_addition:\n",
    "        return (f\"{a}+{b}\", str(a + b))\n",
    "    else:\n",
    "        return (f\"{a}-{b}\", str(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.048965354Z",
     "start_time": "2023-10-06T09:28:45.302336051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('649+864', '1513')"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123456\n",
    "random_formula(3, rng=Random(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `rng` argument allows us to reproduce the same random numbers, which you can verify by running the code below multiple times. But if you change the seed to `None` then the random generator is initialized differently each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.049399607Z",
     "start_time": "2023-10-06T09:28:45.310267095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649+864 = 1513\n",
      "-940-819 = -1759\n",
      "954-2 = 952\n",
      "-896-274 = -1170\n",
      "-762-954 = -1716\n"
     ]
    }
   ],
   "source": [
    "def random_formulas(complexity, signed, count, seed):\n",
    "    \"\"\"\n",
    "    Iterator that yields the given count of random formulas\n",
    "    \"\"\"\n",
    "    rng = Random(seed)\n",
    "    for i in range(count):\n",
    "        yield random_formula(complexity, signed, rng=rng)\n",
    "\n",
    "\n",
    "for q, a in random_formulas(3, True, 5, seed):\n",
    "    print(f'{q} = {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...We are going to treat these expressions as sequences of tokens, where each character is a token. In addition we will need tokens to denote begin-of-sequence and end-of-sequence, as well as padding, for which we will use `'<bos>'`, `'<eos>'`, and `'<pad>'` respectively, as is done in the book.\n",
    "\n",
    "[d2l chapter 9.2](https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html) includes an example of tokenizing a string, and it also defines a `Vocab` class that handles converting the tokens to numbers.\n",
    "\n",
    "For this dataset we know beforehand what the vocabulary will be.\n",
    "\n",
    "### Creating a vocabulary\n",
    "\n",
    "**(a) What are the tokens in this dataset? Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.068664060Z",
     "start_time": "2023-10-06T09:28:45.317345851Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = d2l.Vocab(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '+', '-'],\n",
    "                  reserved_tokens=['<eos>', '<pad>', '<bos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the vocabulary to double check that it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.089458030Z",
     "start_time": "2023-10-06T09:28:45.332308264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16\n",
      "Vocabulary: ['+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<bos>', '<eos>', '<pad>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', len(vocab))\n",
    "print('Vocabulary:', vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the d2l Vocab class includes a `'<unk>'` token, for handling unknown tokens in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to tokenize and encode formula.\n",
    "\n",
    "**(b) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.112142991Z",
     "start_time": "2023-10-06T09:28:45.339637799Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_encode(string: str, vocab=vocab) -> List[int]:\n",
    "    chars = [*string]\n",
    "    ret = vocab[chars]\n",
    "    ret.append(vocab['<eos>'])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a random formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.161435206Z",
     "start_time": "2023-10-06T09:28:45.348243915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question 649+864 and answer 1513\n",
      "are encoded as [8, 6, 11, 0, 10, 8, 6, 13] and [3, 7, 3, 5, 13]\n"
     ]
    }
   ],
   "source": [
    "q, a = random_formula(3, rng=Random(seed))\n",
    "print('The question', q, 'and answer', a)\n",
    "print('are encoded as', tokenize_and_encode(q), 'and', tokenize_and_encode(a))\n",
    "\n",
    "# Check tokenize_and_encode\n",
    "assert ''.join(vocab.to_tokens(tokenize_and_encode(q))) == q + '<eos>'\n",
    "assert len(tokenize_and_encode(q)) == len(q) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and trimming\n",
    "\n",
    "Next, to be able to work with a whole dataset of these encoded sequences, they all need to be the same length.\n",
    "\n",
    "**(c) Implement the function below that pads or trims the encoded token sequence as needed.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: see [d2l section 10.5.3](http://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#loading-sequences-of-fixed-length) for a very similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.179193296Z",
     "start_time": "2023-10-06T09:28:45.350093204Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_or_trim(tokens: List[int], target_length: int, vocab=vocab):\n",
    "    if len(tokens) > target_length:\n",
    "        return tokens[:target_length]\n",
    "    else:\n",
    "        pad_token = vocab['<pad>']\n",
    "        for _ in range(target_length - len(tokens)):\n",
    "            tokens.append(pad_token)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.179659957Z",
     "start_time": "2023-10-06T09:28:45.355991862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[8, 6, 11, 0, 10, 8, 6, 13, 14, 14]"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad or trim q to get a sequence of 10 tokens\n",
    "pad_or_trim(tokenize_and_encode(q), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.187829999Z",
     "start_time": "2023-10-06T09:28:45.364522602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check pad_or_trim\n",
    "assert len(pad_or_trim([1, 2, 3, 4, 5], 10)) == 10\n",
    "assert len(pad_or_trim(list(range(20)), 10)) == 10\n",
    "assert vocab.to_tokens(pad_or_trim([1, 2, 3, 4, 5], 10)[5:]) == ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], \\\n",
    "    f\"Incorrect padding tokens, found {vocab.to_tokens(pad_or_trim([1, 2, 3, 4, 5], 10)[5:])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating tokens\n",
    "\n",
    "We can use `vocab.to_tokens` to convert the encoded token sequence back to something more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.188159065Z",
     "start_time": "2023-10-06T09:28:45.364699723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['6', '4', '9', '+', '8', '6', '4', '<eos>', '<pad>', '<pad>']"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(pad_or_trim(tokenize_and_encode(q), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we define the `decode_tokens` function to convert entire lists or tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.217973311Z",
     "start_time": "2023-10-06T09:28:45.371546213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5' '1' '3' '+' '1' '3' '2' '3' '<eos>' '<pad>']\n",
      " ['4' '1' '2' '+' '4' '2' '<eos>' '<pad>' '<pad>' '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "def decode_tokens(t, vocab=vocab):\n",
    "    # convert a list, tensor, or array of encoded tokens\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        t = t.detach().cpu().numpy()\n",
    "    t = np.asarray(t)\n",
    "    return np.asarray(vocab.to_tokens(list(t.flatten()))).reshape(*t.shape)\n",
    "\n",
    "\n",
    "# convert all tokens at once\n",
    "print(decode_tokens([pad_or_trim(tokenize_and_encode('513+1323'), 10),\n",
    "                     pad_or_trim(tokenize_and_encode('412+42'), 10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset\n",
    "\n",
    "The most convenient way to use a data generating function for training a neural network is to wrap it in a PyTorch `Dataset`. In this case, we will use an [IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset), which can be used as an iterator to walk over the samples in the dataset.\n",
    "\n",
    "**(d) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.225840512Z",
     "start_time": "2023-10-06T09:28:45.379065838Z"
    }
   },
   "outputs": [],
   "source": [
    "class FormulaDataset(IterableDataset):\n",
    "    def __init__(self, complexity, signed, count, seed=None, vocab=vocab):\n",
    "        self.seed = seed\n",
    "        self.complexity = complexity\n",
    "        self.signed = signed\n",
    "        self.count = count\n",
    "        self.vocab = vocab\n",
    "        self.max_question_length = 2 * complexity + 3\n",
    "        self.max_answer_length = complexity + 2\n",
    "\n",
    "    def __iter__(self):\n",
    "        formulas = random_formulas(self.complexity, self.signed, self.count, self.seed)\n",
    "        for f in formulas:\n",
    "            x_encoded = tokenize_and_encode(f[0], self.vocab)\n",
    "            y_encoded = tokenize_and_encode(f[1], self.vocab)\n",
    "            x_padded = pad_or_trim(x_encoded, self.max_question_length, self.vocab)\n",
    "            y_padded = pad_or_trim(y_encoded, self.max_answer_length, self.vocab)\n",
    "            yield torch.tensor(x_padded), torch.tensor(y_padded)\n",
    "\n",
    "    # Complete the class definition.\n",
    "    #       See the documentation for IterableDataset for examples.\n",
    "    #       Make sure that the values yielded by the iterator are pairs of torch tensors.\n",
    "    #       To create a repeatable dataset, always start with the same random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Define a training set with 10000 formulas and a validation set with 5000 formulas, both with complexity 3.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: make sure that the training and validation set are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.249955672Z",
     "start_time": "2023-10-06T09:28:45.386365594Z"
    }
   },
   "outputs": [],
   "source": [
    "complexity = 3\n",
    "signed = True\n",
    "train_data = FormulaDataset(3, True, 10000, seed, vocab)\n",
    "val_data = FormulaDataset(3, True, 5000, seed + 1, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we wrap each dataset in a `DataLoader` to create minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.269815078Z",
     "start_time": "2023-10-06T09:28:45.388877432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "batch_size = 125\n",
    "data_loaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_data, batch_size=batch_size),\n",
    "    'val': torch.utils.data.DataLoader(val_data, batch_size=batch_size),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.542394083Z",
     "start_time": "2023-10-06T09:28:45.400212606Z"
    }
   },
   "outputs": [],
   "source": [
    "# The code below checks that the datasets are defined correctly\n",
    "train_loader = data_loaders['train']\n",
    "val_loader = data_loaders['val']\n",
    "\n",
    "from typing import Tuple\n",
    "from typing_extensions import assert_type\n",
    "\n",
    "for (name, loader), expected_size in zip(data_loaders.items(), [10000, 5000]):\n",
    "    first_batch = next(iter(loader))\n",
    "    assert len(first_batch) == 2, \\\n",
    "        f\"The {name} dataset should yield (question, answer) pairs when iterated over.\"\n",
    "    assert torch.is_tensor(first_batch[0]), \\\n",
    "        f\"The questions in the {name} dataset should be torch.tensors\"\n",
    "    assert tuple(first_batch[0].shape) == (batch_size, 2 * complexity + 3), \\\n",
    "        f\"The questions in the {name} dataset should be of size (batch_size, max_question_length), i.e. {batch_size, 2 * complexity + 3}, found {tuple(first_batch[0].shape)}\"\n",
    "    assert first_batch[0].dtype in [torch.int32, torch.int64], \\\n",
    "        f\"The questions in the {name} dataset should be encoded as integers, found {first_batch[0].dtype}\"\n",
    "    assert torch.equal(next(iter(loader))[0], next(iter(loader))[0]), \\\n",
    "        f\"The {name} dataset should be deterministic, it should produce the same data each time\"\n",
    "    assert all([len(batch[0]) == batch_size for batch in iter(loader)]), \\\n",
    "        f\"Batches should all have the right size. Perhaps the batch size does not evenly divide the dataset size?\"\n",
    "    assert sum([len(batch[0]) for batch in iter(loader)]) == expected_size, \\\n",
    "        f\"{name} dataset does not have the right size, expected {expected_size}, found {sum([len(batch[0]) for batch in iter(loader)])}.\"\n",
    "assert not torch.equal(next(iter(train_loader))[0], next(iter(val_loader))[0]), \\\n",
    "    \"The training data and validation data should not be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Transformer inputs (10 points)\n",
    "\n",
    "There is a detailed description of the transformer model in [chapter 11 of the d2l book](http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html). We will not use most the code from the book, and instead use [PyTorch's built-in Transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n",
    "\n",
    "However, some details we still need to implement ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks\n",
    "\n",
    "Training a transformer uses masked self-attention, so we need some masks. Here are two functions that make these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.542731748Z",
     "start_time": "2023-10-06T09:28:45.916749496Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(size, device=device):\n",
    "    \"\"\"\n",
    "    Mask that indicates that tokens at a position are not allowed to attend to\n",
    "    tokens in subsequent positions.\n",
    "    \"\"\"\n",
    "    mask = (torch.tril(torch.ones((size, size), device=device))) == 0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_padding_mask(tokens, padding_token):\n",
    "    \"\"\"\n",
    "    Mask that indicates which tokens should be ignored because they are padding.\n",
    "    \"\"\"\n",
    "    return tokens == torch.tensor(padding_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Generate a padding mask for a random encoded token string.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: make sure that `tokens` is a torch.tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.543001527Z",
     "start_time": "2023-10-06T09:28:45.916891416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8,  6, 11,  0, 10,  8,  6, 13, 14, 14])\n",
      "['6' '4' '9' '+' '8' '6' '4' '<eos>' '<pad>' '<pad>']\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "q, a = random_formula(3, rng=Random(seed))\n",
    "tokens = torch.tensor(pad_or_trim(tokenize_and_encode(q, vocab), 10))\n",
    "padding_mask = generate_padding_mask(tokens, vocab['<pad>'])\n",
    "print(tokens)\n",
    "print(decode_tokens(tokens))\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.543103076Z",
     "start_time": "2023-10-06T09:28:45.916971728Z"
    }
   },
   "outputs": [],
   "source": [
    "# More tests\n",
    "assert list(generate_padding_mask(torch.tensor(pad_or_trim(tokenize_and_encode(\"1+1\"), 8)), vocab['<pad>'])) == [\n",
    "    False] * 4 + [True] * 4, \"Something is wrong with generate_padding_mask\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) How will this mask be used by a transformer?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masks make sure the transformer network does not put its attention to input that it should not use for training, e.g., because it contains the answer. This would happen, because we use self-attention in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes the first batch of data from the training set, and it generates a shifted version of the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.543285163Z",
     "start_time": "2023-10-06T09:28:45.917011367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '5' '1' '3' '<eos>']\n",
      " ['-' '1' '7' '5' '9']\n",
      " ['9' '5' '2' '<eos>' '<pad>']\n",
      " ['-' '1' '1' '7' '0']\n",
      " ['-' '1' '7' '1' '6']]\n",
      "[['<bos>' '1' '5' '1' '3']\n",
      " ['<bos>' '-' '1' '7' '5']\n",
      " ['<bos>' '9' '5' '2' '<eos>']\n",
      " ['<bos>' '-' '1' '1' '7']\n",
      " ['<bos>' '-' '1' '7' '1']]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "bos = torch.tensor(vocab['<bos>']).expand(y.shape[0], 1)\n",
    "y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n",
    "\n",
    "# print the first five samples\n",
    "print(decode_tokens(y)[:5])\n",
    "print(decode_tokens(y_prev)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Look at the values for the example above. What is `y_prev` used for during training of a transformer model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_prev` will be used as queries, as we use self-attention in our model. `y` will then serve as the correct answers to the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Why do some rows of `y_prev` end in `'<eos>'`, but not all? Is this a problem?<span style=\"float:right\"> (1 point)</span>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some sequences were already full, shifting it by one will \"throw out\" the last, i.e., the `<eos>` token. This is not a problem, as we only use `y_prev` as queries and not as values. Therefore, the only thing we would predict after a `<eos>` token would always be the `<pad>` token, which we are not interested in, anyway. What is interesting, though, is to predict when a sentence ends, which we can still verify to be correct as `y` keeps the correct answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below illustrates what the output of `generate_square_subsequent_mask` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.543453691Z",
     "start_time": "2023-10-06T09:28:45.917094063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "square_subsequent_mask = generate_square_subsequent_mask(y.shape[1])\n",
    "\n",
    "print(square_subsequent_mask.shape)\n",
    "print(square_subsequent_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) How and why should this mask be used? State your answer in terms of `x`,  `y` and/or `y_prev`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "Using the square_subsequent_mask to mask `y` we make sure that during the learning, the network cannot simply look at the correct answers by looking one value forward. For example, for the first query from `y_prev`, i.e. `<bos>` we do not want the network to simply look at the first value of `y` as this would be the correct value to the query. TODO that's strange: we do allow to see the first value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Give an example where it could make sense to use a different mask in a transformer network, instead of the `square_subsequent_mask`?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "For example, if you want to recover partially lost data of a sequence, i.e., if there are scratches in a CD. Here, we would allow looking at data that is a bit away from the current query in both directions, i.e., we would need a mask that does not allow to look at data close to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Our discrete vocabulary is not suitable as the input for a transformer. We need an embedding function to map our input vocabulary to a continuous, high-dimensional space.\n",
    "\n",
    "We will use the `torch.nn.Embedding` class to for this. As you can read in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding), this class maps each token in our vocabulary to a specific point in embedding space, its embedding vector. We will use this embedding vector as the input features for the next layer of our model.\n",
    "\n",
    "The parameters of the embedding are trainable: the embedding vector of each token is optimized along with the rest of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Define an embedding that maps our vocabulary to a 5-dimensional space.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.543616953Z",
     "start_time": "2023-10-06T09:28:45.917170336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(16, 5)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(len(vocab), 5)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the embedding to some sequences from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.543810061Z",
     "start_time": "2023-10-06T09:28:45.922670097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  6, 11,  0, 10,  8,  6, 13, 14],\n",
      "        [ 1, 11,  6,  2,  1, 10,  3, 11, 13],\n",
      "        [11,  7,  6,  1,  4, 13, 14, 14, 14]])\n",
      "tensor([[[-0.1344,  2.8923,  0.0830,  0.6552, -1.8021],\n",
      "         [-0.0687,  0.2428, -0.1075, -1.3675, -1.1949],\n",
      "         [-0.6319,  1.1867,  0.6901,  0.7067, -1.3013],\n",
      "         [ 0.1813, -0.2179, -0.1479, -1.0554,  1.5588],\n",
      "         [ 0.3709, -1.8080,  0.1319, -0.4801,  0.1656],\n",
      "         [-0.1344,  2.8923,  0.0830,  0.6552, -1.8021],\n",
      "         [-0.0687,  0.2428, -0.1075, -1.3675, -1.1949],\n",
      "         [ 0.4320,  1.8396, -0.0504, -1.0046, -0.0181],\n",
      "         [-1.7108, -0.6302, -0.5851, -1.1013,  1.2061]],\n",
      "\n",
      "        [[ 0.1810, -1.4532, -1.3207, -0.2025,  0.8716],\n",
      "         [-0.6319,  1.1867,  0.6901,  0.7067, -1.3013],\n",
      "         [-0.0687,  0.2428, -0.1075, -1.3675, -1.1949],\n",
      "         [-0.0300, -0.3053, -0.5900, -0.1692,  3.3946],\n",
      "         [ 0.1810, -1.4532, -1.3207, -0.2025,  0.8716],\n",
      "         [ 0.3709, -1.8080,  0.1319, -0.4801,  0.1656],\n",
      "         [-2.2261, -0.1457, -0.4813,  1.0103, -1.6498],\n",
      "         [-0.6319,  1.1867,  0.6901,  0.7067, -1.3013],\n",
      "         [ 0.4320,  1.8396, -0.0504, -1.0046, -0.0181]],\n",
      "\n",
      "        [[-0.6319,  1.1867,  0.6901,  0.7067, -1.3013],\n",
      "         [ 0.3615,  1.0489, -0.1106,  0.2518,  2.2641],\n",
      "         [-0.0687,  0.2428, -0.1075, -1.3675, -1.1949],\n",
      "         [ 0.1810, -1.4532, -1.3207, -0.2025,  0.8716],\n",
      "         [ 1.0422,  0.9969,  0.8560,  0.8407, -0.7329],\n",
      "         [ 0.4320,  1.8396, -0.0504, -1.0046, -0.0181],\n",
      "         [-1.7108, -0.6302, -0.5851, -1.1013,  1.2061],\n",
      "         [-1.7108, -0.6302, -0.5851, -1.1013,  1.2061],\n",
      "         [-1.7108, -0.6302, -0.5851, -1.1013,  1.2061]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 9])\n",
      "torch.Size([3, 9, 5])\n"
     ]
    }
   ],
   "source": [
    "# take the first batch\n",
    "x, y = next(iter(train_loader))\n",
    "# take three samples\n",
    "x = x[:3]\n",
    "# print the shapes\n",
    "print(x)\n",
    "print(embedding(x))\n",
    "print(x.shape)\n",
    "print(embedding(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Explain the output shape.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of x is the number of equations x number of tokens in each equation.\n",
    "\n",
    "The shape of the embedding adds another dimension of 5 to represent each encoded character of the equation as a vector of five dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the embedding vectors, or the dimensionality of the embedding space, does not depend on the number of tokens in our vocabulary. We are free to choose an embedding size that fits our problem.\n",
    "\n",
    "For example, let's try an embedding with 2 dimensions, and plot the initial embedding for the tokens in our vocabulary.\n",
    "\n",
    "**(i) Create an embedding with 2 dimensions and plot the embedding for all tokens.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.544067745Z",
     "start_time": "2023-10-06T09:28:45.927357182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"402.802812pt\" height=\"297.190125pt\" viewBox=\"0 0 402.802812 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-10-06T11:28:46.007309</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 297.190125 \nL 402.802812 297.190125 \nL 402.802812 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 38.482813 273.312 \nL 395.602813 273.312 \nL 395.602813 7.2 \nL 38.482813 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path id=\"m82ce624af0\" d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" style=\"stroke: #1f77b4\"/>\n    </defs>\n    <g clip-path=\"url(#pc03ac4465b)\">\n     <use xlink:href=\"#m82ce624af0\" x=\"169.970609\" y=\"250.015786\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"326.621853\" y=\"160.703387\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"379.370085\" y=\"19.296\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"265.763129\" y=\"230.225626\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"352.043799\" y=\"174.686745\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"119.716371\" y=\"187.993803\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"186.487538\" y=\"241.780521\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"186.822209\" y=\"226.081223\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"181.615904\" y=\"170.665265\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"68.421785\" y=\"261.216\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"235.114837\" y=\"258.302419\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"179.210614\" y=\"211.455586\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"79.439078\" y=\"47.315061\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"238.257636\" y=\"199.50132\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"110.923407\" y=\"237.997327\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n     <use xlink:href=\"#m82ce624af0\" x=\"54.71554\" y=\"137.042976\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"me421795666\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#me421795666\" x=\"86.891988\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −1.0 -->\n      <g transform=\"translate(74.750582 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#me421795666\" x=\"158.667404\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −0.5 -->\n      <g transform=\"translate(146.525998 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#me421795666\" x=\"230.44282\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.0 -->\n      <g transform=\"translate(222.491257 287.910437) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#me421795666\" x=\"302.218235\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.5 -->\n      <g transform=\"translate(294.266673 287.910437) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#me421795666\" x=\"373.993651\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1.0 -->\n      <g transform=\"translate(366.042088 287.910437) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path id=\"me643b9a89c\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"270.864437\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- −1.5 -->\n      <g transform=\"translate(7.2 274.663655) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"241.19142\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- −1.0 -->\n      <g transform=\"translate(7.2 244.990638) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"211.518403\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −0.5 -->\n      <g transform=\"translate(7.2 215.317621) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"181.845386\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g transform=\"translate(15.579688 185.644605) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"152.172369\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.5 -->\n      <g transform=\"translate(15.579688 155.971588) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"122.499352\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(15.579688 126.298571) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"92.826335\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.5 -->\n      <g transform=\"translate(15.579688 96.625554) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"63.153318\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2.0 -->\n      <g transform=\"translate(15.579688 66.952537) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#me643b9a89c\" x=\"38.482813\" y=\"33.480301\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 2.5 -->\n      <g transform=\"translate(15.579688 37.27952) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 38.482813 273.312 \nL 38.482813 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 395.602813 273.312 \nL 395.602813 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 38.482812 273.312 \nL 395.602812 273.312 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 38.482812 7.2 \nL 395.602812 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- + -->\n    <g transform=\"translate(175.712643 250.015786) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-2b\" d=\"M 2944 4013 \nL 2944 2272 \nL 4684 2272 \nL 4684 1741 \nL 2944 1741 \nL 2944 0 \nL 2419 0 \nL 2419 1741 \nL 678 1741 \nL 678 2272 \nL 2419 2272 \nL 2419 4013 \nL 2944 4013 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-2b\"/>\n    </g>\n   </g>\n   <g id=\"text_16\">\n    <!-- - -->\n    <g transform=\"translate(332.363887 160.703387) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-2d\"/>\n    </g>\n   </g>\n   <g id=\"text_17\">\n    <!-- 0 -->\n    <g transform=\"translate(385.112118 19.296) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_18\">\n    <!-- 1 -->\n    <g transform=\"translate(271.505163 230.225626) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-31\"/>\n    </g>\n   </g>\n   <g id=\"text_19\">\n    <!-- 2 -->\n    <g transform=\"translate(357.785832 174.686745) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-32\"/>\n    </g>\n   </g>\n   <g id=\"text_20\">\n    <!-- 3 -->\n    <g transform=\"translate(125.458404 187.993803) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-33\"/>\n    </g>\n   </g>\n   <g id=\"text_21\">\n    <!-- 4 -->\n    <g transform=\"translate(192.229572 241.780521) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-34\"/>\n    </g>\n   </g>\n   <g id=\"text_22\">\n    <!-- 5 -->\n    <g transform=\"translate(192.564242 226.081223) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-35\"/>\n    </g>\n   </g>\n   <g id=\"text_23\">\n    <!-- 6 -->\n    <g transform=\"translate(187.357937 170.665265) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-36\"/>\n    </g>\n   </g>\n   <g id=\"text_24\">\n    <!-- 7 -->\n    <g transform=\"translate(74.163819 261.216) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-37\"/>\n    </g>\n   </g>\n   <g id=\"text_25\">\n    <!-- 8 -->\n    <g transform=\"translate(240.85687 258.302419) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-38\"/>\n    </g>\n   </g>\n   <g id=\"text_26\">\n    <!-- 9 -->\n    <g transform=\"translate(184.952647 211.455586) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-39\"/>\n    </g>\n   </g>\n   <g id=\"text_27\">\n    <!-- &lt;bos&gt; -->\n    <g transform=\"translate(85.181111 47.315061) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-3c\" d=\"M 4684 3150 \nL 1459 2003 \nL 4684 863 \nL 4684 294 \nL 678 1747 \nL 678 2266 \nL 4684 3719 \nL 4684 3150 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-3e\" d=\"M 678 3150 \nL 678 3719 \nL 4684 2266 \nL 4684 1747 \nL 678 294 \nL 678 863 \nL 3897 2003 \nL 678 3150 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-3c\"/>\n     <use xlink:href=\"#DejaVuSans-62\" x=\"83.789062\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"147.265625\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"208.447266\"/>\n     <use xlink:href=\"#DejaVuSans-3e\" x=\"260.546875\"/>\n    </g>\n   </g>\n   <g id=\"text_28\">\n    <!-- &lt;eos&gt; -->\n    <g transform=\"translate(243.999669 199.50132) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-3c\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"83.789062\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"145.3125\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"206.494141\"/>\n     <use xlink:href=\"#DejaVuSans-3e\" x=\"258.59375\"/>\n    </g>\n   </g>\n   <g id=\"text_29\">\n    <!-- &lt;pad&gt; -->\n    <g transform=\"translate(116.665441 237.997327) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-3c\"/>\n     <use xlink:href=\"#DejaVuSans-70\" x=\"83.789062\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"147.265625\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"208.544922\"/>\n     <use xlink:href=\"#DejaVuSans-3e\" x=\"272.021484\"/>\n    </g>\n   </g>\n   <g id=\"text_30\">\n    <!-- &lt;unk&gt; -->\n    <g transform=\"translate(60.457573 137.042976) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \nL 1159 4863 \nL 1159 1991 \nL 2875 3500 \nL 3609 3500 \nL 1753 1863 \nL 3688 0 \nL 2938 0 \nL 1159 1709 \nL 1159 0 \nL 581 0 \nL 581 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-3c\"/>\n     <use xlink:href=\"#DejaVuSans-75\" x=\"83.789062\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"147.167969\"/>\n     <use xlink:href=\"#DejaVuSans-6b\" x=\"210.546875\"/>\n     <use xlink:href=\"#DejaVuSans-3e\" x=\"268.457031\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc03ac4465b\">\n   <rect x=\"38.482813\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding = nn.Embedding(len(vocab), 2)\n",
    "\n",
    "# embed all tokens of our vocabulary\n",
    "x = torch.arange(len(vocab))\n",
    "emb = embedding(x).detach().cpu().numpy()\n",
    "\n",
    "plt.scatter(emb[:, 0], emb[:, 1]);\n",
    "for i, token in enumerate(vocab.idx_to_token):\n",
    "    plt.annotate(token, (emb[i, 0] + 0.04, emb[i, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need to balance the complexity of our networks: a larger embedding will increase the number of parameters in our model, but increase the risk of overfitting.\n",
    "\n",
    "**(j) Would this 2-dimensional embedding space be large enough for our problem?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "\n",
    "Yes, two dimensions are probably enough, as the space we have to cover is rather small. Especially, there are only a few different meanings in our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using an embedding, we could also use a simple one-hot encoding to map the words in the vocabulary to feature vectors. However, practical applications of natural language processing never do this. Why not?\n",
    "\n",
    "**(k) Explain the practical advantage of embeddings over one-hot encoding.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "\n",
    "Embeddings are more recourse efficient, as they are encoded as floating point numbers while a one-hot encoding would be a huge vector with the size of the dictionary. This is probably bigger.\n",
    "\n",
    "Additionally, this means that the whole vocabulary needs to be known upfront and cannot be chaned easily afterwards, while you can just add another token into the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 `torch.nn.Transformer` (8 points)\n",
    "\n",
    "<div style=\"float: left\"><a href=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\"><img src=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\" width=\"300\"></a></div>\n",
    "\n",
    "We now have all required inputs for our transformer.\n",
    "\n",
    "Consult the documentation for the [`torch.nn.Transformer`](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) class of PyTorch. This class implements a full Transformer as described in [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf), the paper that introduced this architecture.\n",
    "\n",
    "The `Transformer` class implements the main part of the of the Transformer architecture, shown highlighted in the image on the left (see also Fig. 1 in \"Attention Is All You Need\").\n",
    "\n",
    "For a given input sequence, it applies one or more encoder layers, followed by one or more decoder layers, to compute an output sequence that we can then process further.\n",
    "\n",
    "Because the `Transformer` class takes care of most of the complicated parts of the model, we can concentrate providing the inputs and outputs: the grayed-out areas in the image.\n",
    "\n",
    "Check out the parameters for the `Transformer` class and the inputs and outputs of its `forward` function.\n",
    "<br style=\"clear: both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Which parameter of the `Transformer` class should we base on our embedding?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "\n",
    "The `d_model` param is equal to the dimensions of the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Given fixed input and output dimensions, which parameters of the `Transformer` can we use to change the complexity of our network?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "\n",
    "- nhead\n",
    "- num_encoder_layers\n",
    "- num_decoder_layers\n",
    "- dim_feedforward\n",
    "- bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) When using the `Transformer` class, where should we use the masks that we defined earlier?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "\n",
    "We will need both, the padding and the square_subsequent_mask as an input to the `src_maks` of the `forward` function. This will be achieved by using an OR operation per row of the square_subsequent_mask. This achieves that the network can never look ahead and at the same time will not spend any attention on the padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Complete the code for the TransformerNetwork.<span style=\"float:right\"> (5 points)</span>**\n",
    "\n",
    "Construct a network with the following architecture (see the image in the previous section for an overview):\n",
    "1. An embedding layer that embeds the input tokens into a space of size `dim_hidden`.\n",
    "2. A dropout layer (not shown in the image).\n",
    "3. A [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) with the specified parameters (`dim_hidden`, `num_heads`, `num_layers`, `dim_feedforward`, and `dropout`).<br>Note: you will need to pass `batch_first=True`, to indicate that the first dimension runs over the batch and not over the sequence.\n",
    "4. A final linear prediction layer that takes the output of the transformer to `dim_vocab` possible classes.\n",
    "\n",
    "Don't worry about positional encoding for now, we will add that later.\n",
    "\n",
    "The `forward` function should generate the appropriate masks and combine the layers defined in `__init__` to compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:59:58.928849179Z",
     "start_time": "2023-10-06T09:59:58.882454723Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_vocab=len(vocab), padding_token=vocab['<pad>'],\n",
    "                 num_layers=2, num_heads=4, dim_hidden=64, dim_feedforward=64,\n",
    "                 dropout=0.01, positional_encoding=False):\n",
    "        super().__init__()\n",
    "        self.padding_token = padding_token\n",
    "        # TODO: Your code here.\n",
    "        self.embedding = nn.Embedding(len(vocab), dim_hidden, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer = nn.Transformer(dim_hidden, num_heads, num_layers, num_layers, dim_feedforward, dropout,\n",
    "                                          batch_first=True, device=device)\n",
    "        self.predict = nn.Linear(dim_hidden, dim_vocab, device=device)\n",
    "        if positional_encoding:\n",
    "            self.pos_encoding = PositionalEncoding(dim_hidden)\n",
    "        else:\n",
    "            self.pos_encoding = torch.nn.Identity()\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        pad_mask_src = generate_padding_mask(src, self.padding_token).to(device)\n",
    "        pad_mask_tgt = generate_padding_mask(tgt, self.padding_token).to(device)\n",
    "        subseq_mask_src = generate_square_subsequent_mask(src.shape[1]).to(device)\n",
    "        subseq_mask_tgt = generate_square_subsequent_mask(tgt.shape[1]).to(device)\n",
    "\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        embedded_src = self.embedding(src)\n",
    "        pos_encoded_src = self.pos_encoding(embedded_src)\n",
    "        embedded_tgt = self.embedding(tgt)\n",
    "        pos_encoded_tgt = self.pos_encoding(embedded_tgt)\n",
    "        dropped_src = self.dropout(pos_encoded_src)\n",
    "\n",
    "        transformed = self.transformer(dropped_src, pos_encoded_tgt, tgt_mask=subseq_mask_tgt,\n",
    "                                       src_key_padding_mask=pad_mask_src, tgt_key_padding_mask=pad_mask_tgt,\n",
    "                                       memory_key_padding_mask=pad_mask_src)\n",
    "        return self.predict(transformed)\n",
    "\n",
    "# TODO: Your code here.\n",
    "# Combine self.embedding, self.dropout, self.transformer, self.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Try the transformer with an example batch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.861365599Z",
     "start_time": "2023-10-06T09:28:46.044220532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([125, 9])\n",
      "y.shape torch.Size([125, 5])\n",
      "y_prev.shape torch.Size([125, 5])\n",
      "y_pred.shape torch.Size([125, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "net = TransformerNetwork(dim_feedforward=72)\n",
    "x, y = next(iter(train_loader))\n",
    "bos = torch.tensor(vocab['<bos>']).expand(y.shape[0], 1)\n",
    "y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n",
    "\n",
    "print('x.shape', x.shape)\n",
    "print('y.shape', y.shape)\n",
    "print('y_prev.shape', y_prev.shape)\n",
    "\n",
    "y_pred = net(x, y_prev)\n",
    "print('y_pred.shape', y_pred.shape)\n",
    "\n",
    "# check the shape against what we expected\n",
    "np.testing.assert_equal(list(y_pred.shape), [y.shape[0], y.shape[1], len(vocab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert these predictions to tokens (but they're obviously random):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.861743831Z",
     "start_time": "2023-10-06T09:28:46.084866751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '5' '5' '5' '9']\n",
      " ['1' '4' '5' '5' '5']\n",
      " ['1' '5' '5' '4' '5']\n",
      " ['1' '4' '5' '5' '5']\n",
      " ['1' '4' '5' '5' '5']]\n"
     ]
    }
   ],
   "source": [
    "print(decode_tokens(torch.argmax(y_pred, dim=2))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T10:00:02.134031663Z",
     "start_time": "2023-10-06T10:00:02.100835938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check that the transformer is defined correctly\n",
    "assert isinstance(net.embedding, torch.nn.Embedding)\n",
    "assert isinstance(net.dropout, torch.nn.Dropout)\n",
    "assert isinstance(net.transformer, torch.nn.Transformer)\n",
    "assert isinstance(net.predict, torch.nn.Linear)\n",
    "# Check parameters of transformer\n",
    "assert net.transformer.d_model == 64\n",
    "assert net.transformer.nhead == 4\n",
    "assert net.transformer.batch_first == True\n",
    "assert net.transformer.encoder.num_layers == 2\n",
    "assert net.transformer.decoder.num_layers == 2\n",
    "assert net.transformer.encoder.layers[0].linear1.out_features == 72\n",
    "assert net.dropout.p == 0.01\n",
    "assert net.transformer.encoder.layers[0].dropout.p == 0.01\n",
    "# Check that the forward function behaves correctly\n",
    "net.train(False)\n",
    "assert torch.all(torch.isclose( \\\n",
    "    net(x, y_prev), \\\n",
    "    net(torch.cat((x, torch.tensor(vocab['<pad>']).expand(x.shape[0], 5)), axis=1), y_prev), atol=1e-5)), \\\n",
    "    \"Adding padding to x should not affect the output of the network. Check src_key_padding_mask and memory_key_padding_mask. The former controls self attention to padding tokens in the encoder, the latter controls cross attention from decoder to encoder.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "    net(x, y_prev), \\\n",
    "    net(x, torch.cat((y_prev, torch.tensor(vocab['<pad>']).expand(y.shape[0], 5)), axis=1))[:, :-5], atol=1e-5)), \\\n",
    "    \"Adding padding to y should not affect the output of the network. Check tgt_key_padding_mask.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "    net(x, y_prev)[:, :2], \\\n",
    "    net(x, y_prev[:, :2]), atol=1e-5)), \\\n",
    "    \"The presence of later tokens in y should not affect the output for earlier tokens. Check tgt_mask.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "    net(x, y_prev), \\\n",
    "    net(torch.flip(x, [1]), y_prev), atol=1e-5)), \\\n",
    "    \"Order of x should not matter for a transformer network. Check src_mask.\"\n",
    "assert not torch.all(torch.isclose( \\\n",
    "    net(x, torch.flip(y_prev, [1])), \\\n",
    "    torch.flip(net(x, y_prev), [1]), atol=1e-5)), \\\n",
    "    \"Order of y should matter for a transformer network. Check tgt_mask.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Training (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will base the training code on last week's code. A complication in computing the loss and accuracy are the padding tokens. So, before we work on the training loop itself, we need to update the `accuracy` function so it ingores these `<pad>` tokens. Let's do this in a generic way\n",
    "\n",
    "**(a) Copy the `accuracy` function from last week, and add a parameter `ignore_index`. The tokens with `y == ignore_index` should be ignored.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: you can select elements from a tensor with `some_tensor[include]` where `include` is a tensor of booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.932828432Z",
     "start_time": "2023-10-06T09:28:46.128789798Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y, ignore_index=None):\n",
    "    # Computes the mean accuracy.\n",
    "    # y_hat: raw network output (before sigmoid or softmax)\n",
    "    #        shape (samples, classes)\n",
    "    # y:     shape (samples)\n",
    "    if y_hat.shape[1] == 1:\n",
    "        # binary classification\n",
    "        y_hat = (y_hat[:, 0] > 0).to(y.dtype)\n",
    "    else:\n",
    "        # multi-class classification\n",
    "        y_hat = torch.argmax(y_hat, axis=1).to(y.dtype)\n",
    "    mask = [y != ignore_index]\n",
    "    y = y[mask]\n",
    "    y_hat = y_hat[mask]\n",
    "    correct = (y_hat == y).to(torch.float32)\n",
    "    return torch.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.933117581Z",
     "start_time": "2023-10-06T09:28:46.128988846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the accuracy function.\n",
    "assert accuracy(torch.tensor([[1, 0, 0], [0.4, 0.5, 0.1], [0, 1, 0], [0.4, 0.1, 0.5]]), torch.tensor([0, 1, 2, 2]),\n",
    "                1) == 2 / 3\n",
    "assert accuracy(torch.tensor([[1, 0, 0], [0.4, 0.5, 0.1], [0, 1, 0], [0.4, 0.1, 0.5]]), torch.tensor([0, 1, 2, 2]),\n",
    "                2) == 1\n",
    "assert accuracy(torch.tensor([[1, 0, 0], [0.4, 0.5, 0.1], [0, 1, 0], [0.4, 0.1, 0.5]]), torch.tensor([0, 1, 2, 2]),\n",
    "                3) == 3 / 4\n",
    "assert accuracy(torch.tensor([[1, 0, 0], [0.4, 0.5, 0.1], [0, 1, 0], [0.4, 0.1, 0.5]]), torch.tensor([2, 2, 1, 2]),\n",
    "                2) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropyLoss**(b) Write a training loop for the transformer model.<span style=\"float:right\"> (4 points)</span>**\n",
    "\n",
    "See last week's assignment for inspiration.\n",
    "The code is mostly the same with the following changes:\n",
    " * The cross-entropy loss function and accuracy should ignore all `<pad>` tokens. (Use `ignore_index`, see the [documentation of CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).)\n",
    " * The network expects `y_prev` as an extra input.\n",
    " * The output of the network contains a batch of N samples, with maximum length L, and gives logits over C classes, so it has size (N,L,C). But `CrossEntropyLoss` and `accuracy` expect a tensor of size (N,C,L). You can use [torch.Tensor.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html) to change the output to the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:28:46.933258443Z",
     "start_time": "2023-10-06T09:28:46.133218136Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, data_loaders, epochs=100, lr=0.001, device=device):\n",
    "    \"\"\"\n",
    "    Trains the model net with data from the data_loaders['train'] and data_loaders['val'].\n",
    "    \"\"\"\n",
    "    net = net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    animator = d2l.Animator(xlabel='epoch',\n",
    "                            legend=['train loss', 'train acc', 'validation loss', 'validation acc'],\n",
    "                            figsize=(10, 5))\n",
    "\n",
    "    timer = {'train': d2l.Timer(), 'val': d2l.Timer()}\n",
    "\n",
    "    ignore_index = vocab['<pad>']\n",
    "    bos_token = vocab['<bos>']\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # monitor loss, accuracy, number of samples\n",
    "        metrics = {'train': d2l.Accumulator(3), 'val': d2l.Accumulator(3)}\n",
    "\n",
    "        for phase in ('train', 'val'):\n",
    "            # switch network to train/eval mode\n",
    "            net.train(phase == 'train')\n",
    "\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
    "                timer[phase].start()\n",
    "\n",
    "                # move to device\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # compute prediction\n",
    "                bos = torch.tensor(bos_token).expand(y.shape[0], 1)\n",
    "                bos = bos.to(device)\n",
    "                y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n",
    "                y_hat = net(x, y_prev)\n",
    "                y_hat = torch.transpose(y_hat, 1, 2)\n",
    "\n",
    "                if y_hat.shape[1] == 1:\n",
    "                    # compute binary cross-entropy loss\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()(y_hat[:, 0], y.to(torch.float))\n",
    "                else:\n",
    "                    # compute cross-entropy loss\n",
    "                    loss = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)(y_hat, y)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # compute gradients and update weights\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                metrics[phase].add(loss * x.shape[0],\n",
    "                                   accuracy(y_hat, y, ignore_index=ignore_index) * x.shape[0],\n",
    "                                   x.shape[0])\n",
    "\n",
    "                timer[phase].stop()\n",
    "\n",
    "        animator.add(epoch + 1,\n",
    "                     (metrics['train'][0] / metrics['train'][2],\n",
    "                      metrics['train'][1] / metrics['train'][2],\n",
    "                      metrics['val'][0] / metrics['val'][2],\n",
    "                      metrics['val'][1] / metrics['val'][2]))\n",
    "\n",
    "    train_loss = metrics['train'][0] / metrics['train'][2]\n",
    "    train_acc = metrics['train'][1] / metrics['train'][2]\n",
    "    val_loss = metrics['val'][0] / metrics['val'][2]\n",
    "    val_acc = metrics['val'][1] / metrics['val'][2]\n",
    "    examples_per_sec = metrics['train'][2] * epochs / timer['train'].sum()\n",
    "\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'val loss {val_loss:.3f}, val acc {val_acc:.3f}')\n",
    "    print(f'{examples_per_sec:.1f} examples/sec '\n",
    "          f'on {str(device)}')\n",
    "    # TODO: Your code here.\n",
    "    # Hint: See assignment 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Train a transformer network. Use 100 epochs with a learning of 0.001<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.105, train acc 0.613, val loss 1.859, val acc 0.441\n",
      "15040.6 examples/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"595.303125pt\" height=\"321.95625pt\" viewBox=\"0 0 595.303125 321.95625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-10-06T11:32:10.449143</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 321.95625 \nL 595.303125 321.95625 \nL 595.303125 0 \nL -0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 284.4 \nL 588.103125 284.4 \nL 588.103125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 50.342794 284.4 \nL 50.342794 7.2 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"mf5312e3437\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mf5312e3437\" x=\"50.342794\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(47.161544 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 152.822133 284.4 \nL 152.822133 7.2 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mf5312e3437\" x=\"152.822133\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(146.459633 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 255.301472 284.4 \nL 255.301472 7.2 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mf5312e3437\" x=\"255.301472\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(248.938972 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 357.780811 284.4 \nL 357.780811 7.2 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mf5312e3437\" x=\"357.780811\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(351.418311 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 460.26015 284.4 \nL 460.26015 7.2 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mf5312e3437\" x=\"460.26015\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(453.89765 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 562.739489 284.4 \nL 562.739489 7.2 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mf5312e3437\" x=\"562.739489\" y=\"284.4\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(553.195739 298.998438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(293.875 312.676562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path d=\"M 30.103125 268.167597 \nL 588.103125 268.167597 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path id=\"m9cc6c783bc\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"268.167597\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 271.966815) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path d=\"M 30.103125 236.334383 \nL 588.103125 236.334383 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"236.334383\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 240.133602) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path d=\"M 30.103125 204.50117 \nL 588.103125 204.50117 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"204.50117\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 208.300389) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path d=\"M 30.103125 172.667957 \nL 588.103125 172.667957 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"172.667957\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 176.467175) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <path d=\"M 30.103125 140.834743 \nL 588.103125 140.834743 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"140.834743\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.2 -->\n      <g transform=\"translate(7.2 144.633962) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_23\">\n      <path d=\"M 30.103125 109.00153 \nL 588.103125 109.00153 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"109.00153\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.4 -->\n      <g transform=\"translate(7.2 112.800749) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <path d=\"M 30.103125 77.168317 \nL 588.103125 77.168317 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"77.168317\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.6 -->\n      <g transform=\"translate(7.2 80.967535) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_27\">\n      <path d=\"M 30.103125 45.335103 \nL 588.103125 45.335103 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"45.335103\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.8 -->\n      <g transform=\"translate(7.2 49.134322) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_29\">\n      <path d=\"M 30.103125 13.50189 \nL 588.103125 13.50189 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#m9cc6c783bc\" x=\"30.103125\" y=\"13.50189\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 17.301109) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 55.466761 19.8 \nL 60.590728 56.478242 \nL 65.714695 63.855157 \nL 70.838662 68.463136 \nL 75.962629 69.950868 \nL 81.086596 72.639837 \nL 86.210563 74.519727 \nL 91.33453 76.014033 \nL 96.458497 77.27199 \nL 101.582464 79.262056 \nL 106.706431 80.092632 \nL 111.830398 82.534868 \nL 116.954365 83.716333 \nL 122.078332 84.861249 \nL 127.202299 86.818017 \nL 132.326265 88.051754 \nL 137.450232 89.13238 \nL 142.574199 90.219241 \nL 147.698166 91.036484 \nL 152.822133 93.373788 \nL 157.9461 93.635172 \nL 163.070067 96.131672 \nL 168.194034 95.542098 \nL 173.318001 96.159334 \nL 178.441968 97.746078 \nL 183.565935 98.439616 \nL 188.689902 100.63991 \nL 193.813869 101.464771 \nL 198.937836 102.338498 \nL 204.061803 101.880098 \nL 209.18577 104.288937 \nL 214.309737 104.546326 \nL 219.433704 107.019134 \nL 224.55767 108.002411 \nL 229.681637 108.500377 \nL 234.805604 109.94254 \nL 239.929571 109.82978 \nL 245.053538 112.02423 \nL 250.177505 112.181095 \nL 255.301472 114.558002 \nL 260.425439 114.690939 \nL 265.549406 115.578619 \nL 270.673373 117.082457 \nL 275.79734 117.379634 \nL 280.921307 119.059949 \nL 286.045274 120.196143 \nL 291.169241 120.654113 \nL 296.293208 121.499544 \nL 301.417175 122.2551 \nL 306.541142 123.946327 \nL 311.665108 123.994102 \nL 316.789075 125.107811 \nL 321.913042 125.843465 \nL 327.037009 126.04827 \nL 332.160976 127.53991 \nL 337.284943 128.429721 \nL 342.40891 129.770233 \nL 347.532877 130.13366 \nL 352.656844 131.031882 \nL 357.780811 131.384588 \nL 362.904778 131.745461 \nL 368.028745 133.22852 \nL 373.152712 133.162908 \nL 378.276679 133.848694 \nL 383.400646 135.251162 \nL 388.524613 136.351596 \nL 393.64858 137.433378 \nL 398.772546 136.793834 \nL 403.896513 138.321484 \nL 409.02048 138.695196 \nL 414.144447 140.161422 \nL 419.268414 139.815903 \nL 424.392381 141.719122 \nL 429.516348 140.582042 \nL 434.640315 142.426033 \nL 439.764282 142.955329 \nL 444.888249 144.209773 \nL 450.012216 144.283286 \nL 455.136183 145.32796 \nL 460.26015 146.165696 \nL 465.384117 145.951065 \nL 470.508084 147.199104 \nL 475.632051 147.480077 \nL 480.756018 148.31962 \nL 485.879985 148.405875 \nL 491.003951 149.114699 \nL 496.127918 149.769253 \nL 501.251885 150.413483 \nL 506.375852 151.564357 \nL 511.499819 151.913517 \nL 516.623786 151.28288 \nL 521.747753 152.571556 \nL 526.87172 153.07324 \nL 531.995687 154.026174 \nL 537.119654 153.934668 \nL 542.243621 154.94628 \nL 547.367588 155.679621 \nL 552.491555 155.810764 \nL 557.615522 155.294811 \nL 562.739489 155.935828 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 55.466761 271.8 \nL 60.590728 267.101766 \nL 65.714695 266.55655 \nL 70.838662 265.690214 \nL 75.962629 264.792778 \nL 81.086596 264.355578 \nL 86.210563 263.545273 \nL 91.33453 262.935888 \nL 96.458497 262.512592 \nL 101.582464 262.160438 \nL 106.706431 261.360006 \nL 111.830398 260.962688 \nL 116.954365 260.725097 \nL 122.078332 260.418828 \nL 127.202299 259.744707 \nL 132.326265 259.404501 \nL 137.450232 258.769649 \nL 142.574199 258.108513 \nL 147.698166 258.039201 \nL 152.822133 257.395578 \nL 157.9461 257.102083 \nL 163.070067 256.07378 \nL 168.194034 255.869053 \nL 173.318001 255.988954 \nL 178.441968 255.234591 \nL 183.565935 255.370838 \nL 188.689902 254.21469 \nL 193.813869 253.587003 \nL 198.937836 253.401664 \nL 204.061803 253.579819 \nL 209.18577 252.456066 \nL 214.309737 252.571744 \nL 219.433704 251.900547 \nL 224.55767 251.249837 \nL 229.681637 250.912714 \nL 234.805604 250.582609 \nL 239.929571 250.88248 \nL 245.053538 249.661967 \nL 250.177505 249.944656 \nL 255.301472 248.935651 \nL 260.425439 248.861177 \nL 265.549406 248.491631 \nL 270.673373 248.113651 \nL 275.79734 247.751964 \nL 280.921307 247.089167 \nL 286.045274 246.898286 \nL 291.169241 246.528916 \nL 296.293208 246.240832 \nL 301.417175 246.12122 \nL 306.541142 245.742626 \nL 311.665108 245.405001 \nL 316.789075 245.135896 \nL 321.913042 244.814687 \nL 327.037009 244.669513 \nL 332.160976 243.947563 \nL 337.284943 243.748381 \nL 342.40891 243.531976 \nL 347.532877 243.049522 \nL 352.656844 242.340264 \nL 357.780811 242.719864 \nL 362.904778 242.598575 \nL 368.028745 241.779005 \nL 373.152712 241.960119 \nL 378.276679 241.724902 \nL 383.400646 241.337135 \nL 388.524613 240.937251 \nL 393.64858 240.272482 \nL 398.772546 240.400751 \nL 403.896513 240.165533 \nL 409.02048 240.100038 \nL 414.144447 239.806566 \nL 419.268414 239.687691 \nL 424.392381 238.968382 \nL 429.516348 239.013466 \nL 434.640315 238.60517 \nL 439.764282 238.591585 \nL 444.888249 238.193251 \nL 450.012216 237.955092 \nL 455.136183 237.809162 \nL 460.26015 236.969431 \nL 465.384117 237.365932 \nL 470.508084 237.397144 \nL 475.632051 237.0785 \nL 480.756018 236.744293 \nL 485.879985 236.647509 \nL 491.003951 236.527892 \nL 496.127918 236.072496 \nL 501.251885 236.084039 \nL 506.375852 235.537394 \nL 511.499819 235.529948 \nL 516.623786 235.901194 \nL 521.747753 234.956726 \nL 526.87172 235.243032 \nL 531.995687 234.512667 \nL 537.119654 234.804845 \nL 542.243621 234.272838 \nL 547.367588 234.305108 \nL 552.491555 234.016179 \nL 557.615522 234.252338 \nL 562.739489 234.241224 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 55.466761 52.800024 \nL 60.590728 64.525577 \nL 65.714695 68.89609 \nL 70.838662 71.9282 \nL 75.962629 68.151262 \nL 81.086596 74.900309 \nL 86.210563 74.597225 \nL 91.33453 74.125083 \nL 96.458497 77.604625 \nL 101.582464 76.795697 \nL 106.706431 81.043704 \nL 111.830398 81.329724 \nL 116.954365 81.605411 \nL 122.078332 82.554797 \nL 127.202299 82.828739 \nL 132.326265 82.666504 \nL 137.450232 82.983449 \nL 142.574199 83.074239 \nL 147.698166 82.599296 \nL 152.822133 82.430391 \nL 157.9461 83.284168 \nL 163.070067 82.077069 \nL 168.194034 82.600636 \nL 173.318001 81.489523 \nL 178.441968 82.511088 \nL 183.565935 81.956054 \nL 188.689902 81.941877 \nL 193.813869 81.078827 \nL 198.937836 80.595769 \nL 204.061803 81.240286 \nL 209.18577 78.72496 \nL 214.309737 80.509053 \nL 219.433704 80.090031 \nL 224.55767 79.354785 \nL 229.681637 78.357617 \nL 234.805604 78.444917 \nL 239.929571 78.711671 \nL 245.053538 73.957449 \nL 250.177505 77.308263 \nL 255.301472 76.438332 \nL 260.425439 76.364742 \nL 265.549406 74.816975 \nL 270.673373 74.502072 \nL 275.79734 73.276448 \nL 280.921307 72.67596 \nL 286.045274 71.104278 \nL 291.169241 72.462292 \nL 296.293208 70.821794 \nL 301.417175 71.647872 \nL 306.541142 68.606705 \nL 311.665108 68.670266 \nL 316.789075 67.921332 \nL 321.913042 68.725324 \nL 327.037009 68.268179 \nL 332.160976 66.955505 \nL 337.284943 65.838779 \nL 342.40891 65.969103 \nL 347.532877 64.772975 \nL 352.656844 63.755973 \nL 357.780811 62.344818 \nL 362.904778 63.753071 \nL 368.028745 62.534961 \nL 373.152712 61.91891 \nL 378.276679 59.436452 \nL 383.400646 61.080316 \nL 388.524613 58.771991 \nL 393.64858 58.08702 \nL 398.772546 56.791514 \nL 403.896513 57.163914 \nL 409.02048 57.183703 \nL 414.144447 55.24845 \nL 419.268414 56.056309 \nL 424.392381 55.945347 \nL 429.516348 55.505633 \nL 434.640315 53.610751 \nL 439.764282 51.517211 \nL 444.888249 52.272139 \nL 450.012216 51.908904 \nL 455.136183 50.733003 \nL 460.26015 47.948727 \nL 465.384117 48.684492 \nL 470.508084 47.573296 \nL 475.632051 46.575387 \nL 480.756018 47.858755 \nL 485.879985 44.946067 \nL 491.003951 43.523209 \nL 496.127918 44.050199 \nL 501.251885 41.562675 \nL 506.375852 41.620642 \nL 511.499819 38.536789 \nL 516.623786 41.175328 \nL 521.747753 40.174805 \nL 526.87172 40.570044 \nL 531.995687 40.016144 \nL 537.119654 38.627377 \nL 542.243621 37.578336 \nL 547.367588 37.225424 \nL 552.491555 33.662794 \nL 557.615522 35.096412 \nL 562.739489 36.022628 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 55.466761 267.485513 \nL 60.590728 267.167316 \nL 65.714695 267.148208 \nL 70.838662 265.892292 \nL 75.962629 266.207693 \nL 81.086596 264.47322 \nL 86.210563 264.547417 \nL 91.33453 263.912972 \nL 96.458497 263.410009 \nL 101.582464 263.444414 \nL 106.706431 263.072556 \nL 111.830398 262.699561 \nL 116.954365 262.496731 \nL 122.078332 261.969739 \nL 127.202299 261.829009 \nL 132.326265 261.843641 \nL 137.450232 262.004096 \nL 142.574199 261.73476 \nL 147.698166 261.989576 \nL 152.822133 261.707439 \nL 157.9461 261.154119 \nL 163.070067 261.863303 \nL 168.194034 261.689311 \nL 173.318001 261.892468 \nL 178.441968 261.761909 \nL 183.565935 261.299297 \nL 188.689902 261.539201 \nL 193.813869 261.410858 \nL 198.937836 261.476413 \nL 204.061803 261.092935 \nL 209.18577 261.378189 \nL 214.309737 260.872581 \nL 219.433704 260.862019 \nL 224.55767 260.81144 \nL 229.681637 261.198562 \nL 234.805604 261.051378 \nL 239.929571 260.846593 \nL 245.053538 261.232651 \nL 250.177505 261.165247 \nL 255.301472 261.229836 \nL 260.425439 261.065166 \nL 265.549406 261.106305 \nL 270.673373 261.117971 \nL 275.79734 261.488778 \nL 280.921307 261.367877 \nL 286.045274 261.055986 \nL 291.169241 261.147947 \nL 296.293208 261.372502 \nL 301.417175 261.079435 \nL 306.541142 261.240659 \nL 311.665108 261.36789 \nL 316.789075 260.846915 \nL 321.913042 261.478197 \nL 327.037009 261.44441 \nL 332.160976 261.372661 \nL 337.284943 261.406163 \nL 342.40891 261.536583 \nL 347.532877 261.799862 \nL 352.656844 262.008771 \nL 357.780811 261.641039 \nL 362.904778 261.241384 \nL 368.028745 261.881645 \nL 373.152712 261.364173 \nL 378.276679 261.778182 \nL 383.400646 261.394193 \nL 388.524613 261.69053 \nL 393.64858 261.110836 \nL 398.772546 261.573008 \nL 403.896513 261.668961 \nL 409.02048 261.534131 \nL 414.144447 261.712618 \nL 419.268414 261.050661 \nL 424.392381 261.445428 \nL 429.516348 261.301055 \nL 434.640315 261.217588 \nL 439.764282 261.717915 \nL 444.888249 261.592722 \nL 450.012216 261.648445 \nL 455.136183 261.907926 \nL 460.26015 261.788572 \nL 465.384117 262.162236 \nL 470.508084 261.882232 \nL 475.632051 261.405839 \nL 480.756018 261.794141 \nL 485.879985 262.081534 \nL 491.003951 261.511874 \nL 496.127918 261.872322 \nL 501.251885 262.738 \nL 506.375852 262.291241 \nL 511.499819 262.303062 \nL 516.623786 262.521947 \nL 521.747753 262.5489 \nL 526.87172 261.572186 \nL 531.995687 262.346731 \nL 537.119654 262.496375 \nL 542.243621 262.145147 \nL 547.367588 262.375726 \nL 552.491555 262.357661 \nL 557.615522 262.316216 \nL 562.739489 261.576003 \n\" clip-path=\"url(#p39497a0b1c)\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 284.4 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 588.103125 284.4 \nL 588.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 284.4 \nL 588.103125 284.4 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 588.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 37.103125 176.65625 \nL 140.83125 176.65625 \nQ 142.83125 176.65625 142.83125 174.65625 \nL 142.83125 116.94375 \nQ 142.83125 114.94375 140.83125 114.94375 \nL 37.103125 114.94375 \nQ 35.103125 114.94375 35.103125 116.94375 \nL 35.103125 174.65625 \nQ 35.103125 176.65625 37.103125 176.65625 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_35\">\n     <path d=\"M 39.103125 123.042188 \nL 49.103125 123.042188 \nL 59.103125 123.042188 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- train loss -->\n     <g transform=\"translate(67.103125 126.542188) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n     </g>\n    </g>\n    <g id=\"line2d_36\">\n     <path d=\"M 39.103125 137.720313 \nL 49.103125 137.720313 \nL 59.103125 137.720313 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- train acc -->\n     <g transform=\"translate(67.103125 141.220313) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"325.830078\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"380.810547\"/>\n     </g>\n    </g>\n    <g id=\"line2d_37\">\n     <path d=\"M 39.103125 152.398438 \nL 49.103125 152.398438 \nL 59.103125 152.398438 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- validation loss -->\n     <g transform=\"translate(67.103125 155.898438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"239.501953\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"300.78125\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"339.990234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"367.773438\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"428.955078\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"492.333984\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"524.121094\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"551.904297\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"613.085938\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"665.185547\"/>\n     </g>\n    </g>\n    <g id=\"line2d_38\">\n     <path d=\"M 39.103125 167.076563 \nL 49.103125 167.076563 \nL 59.103125 167.076563 \n\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_20\">\n     <!-- validation acc -->\n     <g transform=\"translate(67.103125 170.576563) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"239.501953\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"300.78125\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"339.990234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"367.773438\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"428.955078\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"492.333984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"524.121094\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"585.400391\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"640.380859\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p39497a0b1c\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"558\" height=\"277.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: your answer here\n",
    "network = TransformerNetwork()\n",
    "train(network, data_loaders, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Briefly discuss the results. Has the training converged? Is this a good calculator?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "The model does not perform well, we can see a clear overfitting, and the validation loss is not better than 44%. One reason could be that the network does not take the order of the input into account yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T10:00:07.774658519Z",
     "start_time": "2023-10-06T10:00:07.762590909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 123+123=246\n",
      "  y_pred[0] tensor([[ 0.0036,  0.3788,  0.2729,  0.7813, -0.1294, -0.1775,  0.2249, -0.0670,\n",
      "         -0.2475, -0.7482,  0.0692, -0.0548,  0.0944,  0.2460, -0.1944, -0.2569],\n",
      "        [-0.3145,  1.0153,  0.1984, -1.0173, -0.4802, -0.0532,  0.7001,  0.3681,\n",
      "         -0.3709, -0.0644,  0.0048,  0.8493, -0.0431, -0.5333, -0.2995,  0.0336],\n",
      "        [ 0.4614,  1.0251,  0.1111, -1.1674, -0.1335,  0.0293,  0.8700,  0.3954,\n",
      "         -0.0228, -0.4024,  1.0670,  0.0971, -0.2289,  0.0279,  0.2616,  1.0788],\n",
      "        [ 0.6096, -0.6405, -0.2874, -0.7897,  0.0880, -0.2788,  0.6925,  0.4694,\n",
      "         -0.1693, -0.3413,  0.1190,  0.1897,  0.3552, -0.8455,  0.1335,  0.7133]],\n",
      "       device='cuda:0')\n",
      "  encoded tensor([[ 3,  1, 15, 15]], device='cuda:0')\n",
      "  tokens [['1' '-' '<unk>' '<unk>']]\n",
      "\n",
      "For 321+321=642\n",
      "  y_pred[0] tensor([[ 0.0036,  0.3788,  0.2729,  0.7813, -0.1294, -0.1775,  0.2249, -0.0670,\n",
      "         -0.2475, -0.7482,  0.0692, -0.0548,  0.0944,  0.2460, -0.1944, -0.2569],\n",
      "        [ 0.2663, -0.1674, -0.0501, -0.9711, -0.1657, -0.2318,  0.7796,  0.0751,\n",
      "         -0.3907, -0.3162, -0.0472,  0.2871,  0.7249, -0.6338,  0.0285,  0.2696],\n",
      "        [ 0.2514,  1.6454,  0.6387, -1.1432, -0.5373, -0.1211,  0.9230, -0.0686,\n",
      "         -0.0550, -0.1725,  0.7310, -0.1083,  0.2471,  0.2823,  0.1871,  0.9799],\n",
      "        [-0.2828,  1.0659,  0.4969, -1.1055, -0.5321, -0.1702,  0.8397,  0.2534,\n",
      "         -0.4007,  0.1892, -0.1991,  0.8481,  0.1264, -0.4553, -0.4259,  0.0819]],\n",
      "       device='cuda:0')\n",
      "  encoded tensor([[3, 6, 1, 1]], device='cuda:0')\n",
      "  tokens [['1' '4' '-' '-']]\n"
     ]
    }
   ],
   "source": [
    "def predict(net, q, a):\n",
    "    # Run net to predict the output given the input `q` and y_prev based on `a`.\n",
    "    # Return predicted y\n",
    "    with torch.no_grad():\n",
    "        # TODO: Your code here.\n",
    "        q = torch.tensor(tokenize_and_encode(q, vocab), device=device)\n",
    "        a = torch.tensor(tokenize_and_encode(a, vocab), device=device)\n",
    "        a = a.expand(1, a.shape[0])\n",
    "        q = q.expand(1, q.shape[0])\n",
    "    \n",
    "\n",
    "        bos = torch.tensor(vocab['<bos>']).expand(a.shape[0], 1)\n",
    "        bos = bos.to(device)\n",
    "        y_prev = torch.cat((bos, a[:, :-1]), axis=1)\n",
    "\n",
    "        return net.forward(q, y_prev)\n",
    "\n",
    "\n",
    "for src, tgt in [('123+123', '246'), ('321+321', '642')]:\n",
    "    print(f'For {src}={tgt}')\n",
    "    y_pred = predict(net, src, tgt)\n",
    "    print('  y_pred[0]', y_pred[0])\n",
    "    print('  encoded', torch.argmax(y_pred, dim=-1))\n",
    "    print('  tokens', decode_tokens(torch.argmax(y_pred, dim=-1)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Compare the predictions for the first element of y with the two different inputs. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "This first element for both predictions is the same. This happens because the input looks the same for the network, as it contains the same tokens in a different order. As the network does not have a sense for the order, the prediction is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Does the validation accuracy estimate how often the model is able to answers formulas correctly? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) If the forward function takes the shifted output `y_prev` as input, how can we use it if we don't know the output yet?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "We can feed it the output we have generated so far for every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Positional encoding (5 points)\n",
    "\n",
    "We did not yet include positional encoding in the network.\n",
    "PyTorch does not include such an encoder, so here we copied the code from the book (slightly modified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T10:00:12.413910180Z",
     "start_time": "2023-10-06T10:00:12.409048671Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "\n",
    "    def __init__(self, num_hiddens, max_len=1000):\n",
    "        super().__init__()\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X + self.P[:, :X.shape[1], :].to(X.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Add positional encoding to the TransformerModel.<span style=\"float:right\"> (point given in earlier question)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:42:36.897929391Z",
     "start_time": "2023-10-06T09:42:36.893002078Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: See over there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Construct and train a network with positional encoding<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.093, train acc 0.971, val loss 0.064, val acc 0.982\n",
      "15667.6 examples/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"601.665625pt\" height=\"323.82034pt\" viewBox=\"0 0 601.665625 323.82034\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-10-06T12:02:07.319537</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 323.82034 \nL 601.665625 323.82034 \nL 601.665625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 286.26409 \nL 594.465625 286.26409 \nL 594.465625 9.06409 \nL 36.465625 9.06409 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 56.705294 286.26409 \nL 56.705294 9.06409 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m328c8092a6\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m328c8092a6\" x=\"56.705294\" y=\"286.26409\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(53.524044 300.862527) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 159.184633 286.26409 \nL 159.184633 9.06409 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m328c8092a6\" x=\"159.184633\" y=\"286.26409\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(152.822133 300.862527) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 261.663972 286.26409 \nL 261.663972 9.06409 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m328c8092a6\" x=\"261.663972\" y=\"286.26409\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(255.301472 300.862527) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 364.143311 286.26409 \nL 364.143311 9.06409 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m328c8092a6\" x=\"364.143311\" y=\"286.26409\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(357.780811 300.862527) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 466.62265 286.26409 \nL 466.62265 9.06409 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m328c8092a6\" x=\"466.62265\" y=\"286.26409\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(460.26015 300.862527) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 569.101989 286.26409 \nL 569.101989 9.06409 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m328c8092a6\" x=\"569.101989\" y=\"286.26409\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(559.558239 300.862527) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(300.2375 314.540652) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path d=\"M 36.465625 282.33199 \nL 594.465625 282.33199 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path id=\"meb73dabd12\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"282.33199\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.00 -->\n      <g transform=\"translate(7.2 286.131209) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path d=\"M 36.465625 248.415394 \nL 594.465625 248.415394 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"248.415394\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.25 -->\n      <g transform=\"translate(7.2 252.214613) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path d=\"M 36.465625 214.498797 \nL 594.465625 214.498797 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"214.498797\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.50 -->\n      <g transform=\"translate(7.2 218.298016) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path d=\"M 36.465625 180.582201 \nL 594.465625 180.582201 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"180.582201\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.75 -->\n      <g transform=\"translate(7.2 184.38142) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <path d=\"M 36.465625 146.665604 \nL 594.465625 146.665604 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"146.665604\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.00 -->\n      <g transform=\"translate(7.2 150.464823) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_23\">\n      <path d=\"M 36.465625 112.749008 \nL 594.465625 112.749008 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"112.749008\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.25 -->\n      <g transform=\"translate(7.2 116.548227) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <path d=\"M 36.465625 78.832412 \nL 594.465625 78.832412 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"78.832412\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.50 -->\n      <g transform=\"translate(7.2 82.63163) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_27\">\n      <path d=\"M 36.465625 44.915815 \nL 594.465625 44.915815 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"44.915815\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.75 -->\n      <g transform=\"translate(7.2 48.715034) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_29\">\n      <path d=\"M 36.465625 10.999219 \nL 594.465625 10.999219 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#meb73dabd12\" x=\"36.465625\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2.00 -->\n      <g transform=\"translate(7.2 14.798438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 61.829261 21.66409 \nL 66.953228 53.53173 \nL 72.077195 60.355857 \nL 77.201162 66.155848 \nL 82.325129 72.413157 \nL 87.449096 78.852383 \nL 92.573063 85.489456 \nL 97.69703 91.65755 \nL 102.820997 96.660526 \nL 107.944964 100.074534 \nL 113.068931 102.552093 \nL 118.192898 106.658779 \nL 123.316865 107.958337 \nL 128.440832 110.28939 \nL 133.564799 111.709261 \nL 138.688765 114.310338 \nL 143.812732 116.775692 \nL 148.936699 118.812853 \nL 154.060666 121.177979 \nL 159.184633 125.579652 \nL 164.3086 132.019042 \nL 169.432567 138.910774 \nL 174.556534 146.976072 \nL 179.680501 155.548678 \nL 184.804468 164.336318 \nL 189.928435 172.854079 \nL 195.052402 179.03562 \nL 200.176369 185.424048 \nL 205.300336 190.855302 \nL 210.424303 194.364853 \nL 215.54827 201.622483 \nL 220.672237 203.154888 \nL 225.796204 208.995468 \nL 230.92017 212.423006 \nL 236.044137 216.604447 \nL 241.168104 219.178163 \nL 246.292071 221.847888 \nL 251.416038 226.13404 \nL 256.540005 229.384284 \nL 261.663972 229.396571 \nL 266.787939 233.088113 \nL 271.911906 234.872956 \nL 277.035873 233.085058 \nL 282.15984 237.08893 \nL 287.283807 239.821305 \nL 292.407774 242.197163 \nL 297.531741 244.485129 \nL 302.655708 246.133004 \nL 307.779675 246.589588 \nL 312.903642 246.206319 \nL 318.027608 249.282394 \nL 323.151575 248.635864 \nL 328.275542 250.629608 \nL 333.399509 251.512211 \nL 338.523476 253.675456 \nL 343.647443 251.825679 \nL 348.77141 248.35152 \nL 353.895377 253.837322 \nL 359.019344 254.742094 \nL 364.143311 254.561891 \nL 369.267278 257.970551 \nL 374.391245 257.244554 \nL 379.515212 259.147634 \nL 384.639179 257.855642 \nL 389.763146 259.845963 \nL 394.887113 261.521107 \nL 400.01108 261.503276 \nL 405.135046 262.822667 \nL 410.259013 261.326243 \nL 415.38298 262.600042 \nL 420.506947 263.080368 \nL 425.630914 265.264795 \nL 430.754881 265.477916 \nL 435.878848 264.924543 \nL 441.002815 264.978643 \nL 446.126782 262.617996 \nL 451.250749 266.391308 \nL 456.374716 266.708594 \nL 461.498683 267.410725 \nL 466.62265 266.220735 \nL 471.746617 264.973983 \nL 476.870584 267.691766 \nL 481.994551 268.630316 \nL 487.118518 267.934568 \nL 492.242485 268.742818 \nL 497.366451 264.964686 \nL 502.490418 265.206112 \nL 507.614385 268.215354 \nL 512.738352 269.438765 \nL 517.862319 270.356396 \nL 522.986286 267.649749 \nL 528.110253 268.721833 \nL 533.23422 270.227243 \nL 538.358187 269.914841 \nL 543.482154 268.157597 \nL 548.606121 270.483623 \nL 553.730088 269.766217 \nL 558.854055 270.151871 \nL 563.978022 268.720033 \nL 569.101989 269.695606 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 61.829261 230.29503 \nL 66.953228 226.982883 \nL 72.077195 226.516125 \nL 77.201162 225.838343 \nL 82.325129 224.07203 \nL 87.449096 222.109649 \nL 92.573063 219.91067 \nL 97.69703 217.813391 \nL 102.820997 216.383234 \nL 107.944964 214.98844 \nL 113.068931 213.919715 \nL 118.192898 212.275944 \nL 123.316865 211.870306 \nL 128.440832 210.893758 \nL 133.564799 210.526918 \nL 138.688765 208.942634 \nL 143.812732 208.461886 \nL 148.936699 207.467195 \nL 154.060666 206.745398 \nL 159.184633 204.964968 \nL 164.3086 202.860395 \nL 169.432567 200.182745 \nL 174.556534 197.053184 \nL 179.680501 194.16269 \nL 184.804468 191.033782 \nL 189.928435 187.831735 \nL 195.052402 185.826687 \nL 200.176369 183.559532 \nL 205.300336 181.840806 \nL 210.424303 180.069771 \nL 215.54827 177.429481 \nL 220.672237 176.543678 \nL 225.796204 174.241164 \nL 230.92017 172.728872 \nL 236.044137 170.864105 \nL 241.168104 169.954981 \nL 246.292071 168.442849 \nL 251.416038 167.21515 \nL 256.540005 165.789124 \nL 261.663972 165.570051 \nL 266.787939 164.170538 \nL 271.911906 163.569169 \nL 277.035873 164.038094 \nL 282.15984 162.901594 \nL 287.283807 161.389089 \nL 292.407774 160.733493 \nL 297.531741 159.952987 \nL 302.655708 159.262736 \nL 307.779675 159.046918 \nL 312.903642 159.013957 \nL 318.027608 158.074431 \nL 323.151575 158.317614 \nL 328.275542 157.583203 \nL 333.399509 157.369552 \nL 338.523476 156.484843 \nL 343.647443 156.943995 \nL 348.77141 157.214204 \nL 353.895377 156.141693 \nL 359.019344 155.893342 \nL 364.143311 155.894666 \nL 369.267278 154.697616 \nL 374.391245 154.878592 \nL 379.515212 154.107338 \nL 384.639179 154.401152 \nL 389.763146 153.873109 \nL 394.887113 153.346828 \nL 400.01108 153.182764 \nL 405.135046 152.741613 \nL 410.259013 153.328418 \nL 415.38298 152.892006 \nL 420.506947 152.867504 \nL 425.630914 152.107813 \nL 430.754881 151.974523 \nL 435.878848 151.986816 \nL 441.002815 152.123448 \nL 446.126782 152.863089 \nL 451.250749 151.663576 \nL 456.374716 151.482021 \nL 461.498683 151.35146 \nL 466.62265 151.720449 \nL 471.746617 151.95599 \nL 476.870584 151.183271 \nL 481.994551 150.851954 \nL 487.118518 151.095282 \nL 492.242485 150.915151 \nL 497.366451 151.917641 \nL 502.490418 151.929084 \nL 507.614385 151.093714 \nL 512.738352 150.731952 \nL 517.862319 150.346902 \nL 522.986286 151.0192 \nL 528.110253 150.742626 \nL 533.23422 150.31117 \nL 538.358187 150.455709 \nL 543.482154 150.936808 \nL 548.606121 150.258922 \nL 553.730088 150.572826 \nL 558.854055 150.457933 \nL 563.978022 150.866814 \nL 569.101989 150.550014 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 61.829261 49.393583 \nL 66.953228 58.133125 \nL 72.077195 64.322577 \nL 77.201162 70.941122 \nL 82.325129 77.357327 \nL 87.449096 85.19484 \nL 92.573063 90.140895 \nL 97.69703 97.451014 \nL 102.820997 101.266392 \nL 107.944964 105.562196 \nL 113.068931 107.57145 \nL 118.192898 111.166427 \nL 123.316865 112.791423 \nL 128.440832 112.517963 \nL 133.564799 115.635549 \nL 138.688765 116.293492 \nL 143.812732 119.875843 \nL 148.936699 119.490503 \nL 154.060666 127.326568 \nL 159.184633 131.6317 \nL 164.3086 139.487426 \nL 169.432567 150.745649 \nL 174.556534 159.789406 \nL 179.680501 167.537919 \nL 184.804468 180.125377 \nL 189.928435 189.183074 \nL 195.052402 190.625465 \nL 200.176369 201.105645 \nL 205.300336 205.270931 \nL 210.424303 210.461251 \nL 215.54827 215.900051 \nL 220.672237 217.983396 \nL 225.796204 223.06131 \nL 230.92017 228.42855 \nL 236.044137 228.376068 \nL 241.168104 232.250675 \nL 246.292071 236.772028 \nL 251.416038 238.275832 \nL 256.540005 242.11855 \nL 261.663972 243.653855 \nL 266.787939 245.302571 \nL 271.911906 249.186915 \nL 277.035873 244.45479 \nL 282.15984 249.527211 \nL 287.283807 249.983943 \nL 292.407774 252.500103 \nL 297.531741 254.381054 \nL 302.655708 255.754786 \nL 307.779675 255.668882 \nL 312.903642 258.195079 \nL 318.027608 259.054799 \nL 323.151575 258.530348 \nL 328.275542 258.532619 \nL 333.399509 260.978125 \nL 338.523476 261.073617 \nL 343.647443 262.723634 \nL 348.77141 259.595982 \nL 353.895377 263.41257 \nL 359.019344 263.676303 \nL 364.143311 263.118252 \nL 369.267278 263.742751 \nL 374.391245 266.257821 \nL 379.515212 264.534535 \nL 384.639179 265.695975 \nL 389.763146 267.924053 \nL 394.887113 267.418563 \nL 400.01108 268.204032 \nL 405.135046 266.470886 \nL 410.259013 251.152805 \nL 415.38298 269.983134 \nL 420.506947 269.860223 \nL 425.630914 270.670754 \nL 430.754881 270.878672 \nL 435.878848 270.191656 \nL 441.002815 270.13494 \nL 446.126782 269.423425 \nL 451.250749 271.305507 \nL 456.374716 271.268422 \nL 461.498683 272.652744 \nL 466.62265 269.804077 \nL 471.746617 269.816342 \nL 476.870584 271.31512 \nL 481.994551 269.974994 \nL 487.118518 271.673315 \nL 492.242485 269.905438 \nL 497.366451 263.541437 \nL 502.490418 268.609285 \nL 507.614385 272.311554 \nL 512.738352 272.258216 \nL 517.862319 271.928238 \nL 522.986286 265.477554 \nL 528.110253 272.240251 \nL 533.23422 271.973334 \nL 538.358187 272.145999 \nL 543.482154 272.979665 \nL 548.606121 271.937781 \nL 553.730088 271.746801 \nL 558.854055 272.829628 \nL 563.978022 273.084105 \nL 569.101989 273.66409 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 61.829261 227.359547 \nL 66.953228 227.473961 \nL 72.077195 226.719449 \nL 77.201162 225.639082 \nL 82.325129 223.501259 \nL 87.449096 221.152868 \nL 92.573063 220.161298 \nL 97.69703 217.237429 \nL 102.820997 216.272022 \nL 107.944964 213.966173 \nL 113.068931 213.235494 \nL 118.192898 211.315923 \nL 123.316865 210.759572 \nL 128.440832 211.698069 \nL 133.564799 209.971375 \nL 138.688765 209.56179 \nL 143.812732 208.661604 \nL 148.936699 208.29975 \nL 154.060666 205.134348 \nL 159.184633 203.55756 \nL 164.3086 200.429668 \nL 169.432567 196.394942 \nL 174.556534 192.576307 \nL 179.680501 190.554006 \nL 184.804468 186.153019 \nL 189.928435 182.535077 \nL 195.052402 181.950786 \nL 200.176369 177.611072 \nL 205.300336 176.127354 \nL 210.424303 174.175689 \nL 215.54827 171.569707 \nL 220.672237 171.082381 \nL 225.796204 168.961188 \nL 230.92017 166.018046 \nL 236.044137 166.648419 \nL 241.168104 164.929872 \nL 246.292071 163.402188 \nL 251.416038 162.765042 \nL 256.540005 160.844048 \nL 261.663972 160.546558 \nL 266.787939 159.983771 \nL 271.911906 158.563985 \nL 277.035873 160.2019 \nL 282.15984 158.108832 \nL 287.283807 157.971804 \nL 292.407774 157.067546 \nL 297.531741 156.250869 \nL 302.655708 156.070045 \nL 307.779675 155.782863 \nL 312.903642 154.806149 \nL 318.027608 154.502476 \nL 323.151575 154.799116 \nL 328.275542 154.754197 \nL 333.399509 153.609399 \nL 338.523476 153.714014 \nL 343.647443 152.861123 \nL 348.77141 154.030854 \nL 353.895377 152.732906 \nL 359.019344 152.440615 \nL 364.143311 152.80314 \nL 369.267278 152.494084 \nL 374.391245 151.393248 \nL 379.515212 152.015961 \nL 384.639179 151.620588 \nL 389.763146 151.081315 \nL 394.887113 151.004013 \nL 400.01108 150.977473 \nL 405.135046 151.407951 \nL 410.259013 153.085298 \nL 415.38298 150.382098 \nL 420.506947 150.245194 \nL 425.630914 150.058685 \nL 430.754881 150.063512 \nL 435.878848 150.137395 \nL 441.002815 150.358696 \nL 446.126782 150.538916 \nL 451.250749 150.003706 \nL 456.374716 149.888547 \nL 461.498683 149.343856 \nL 466.62265 150.254754 \nL 471.746617 150.411086 \nL 476.870584 149.679921 \nL 481.994551 150.008349 \nL 487.118518 149.789113 \nL 492.242485 149.86099 \nL 497.366451 151.711182 \nL 502.490418 150.744467 \nL 507.614385 149.501005 \nL 512.738352 149.605033 \nL 517.862319 149.525614 \nL 522.986286 150.213673 \nL 528.110253 149.434695 \nL 533.23422 149.387994 \nL 538.358187 149.381837 \nL 543.482154 149.332968 \nL 548.606121 149.650575 \nL 553.730088 149.681147 \nL 558.854055 149.327628 \nL 563.978022 149.171571 \nL 569.101989 149.062781 \n\" clip-path=\"url(#p15b51f90a4)\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 286.26409 \nL 36.465625 9.06409 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 594.465625 286.26409 \nL 594.465625 9.06409 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 286.26409 \nL 594.465625 286.26409 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 9.06409 \nL 594.465625 9.06409 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 483.7375 75.77659 \nL 587.465625 75.77659 \nQ 589.465625 75.77659 589.465625 73.77659 \nL 589.465625 16.06409 \nQ 589.465625 14.06409 587.465625 14.06409 \nL 483.7375 14.06409 \nQ 481.7375 14.06409 481.7375 16.06409 \nL 481.7375 73.77659 \nQ 481.7375 75.77659 483.7375 75.77659 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_35\">\n     <path d=\"M 485.7375 22.162527 \nL 495.7375 22.162527 \nL 505.7375 22.162527 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- train loss -->\n     <g transform=\"translate(513.7375 25.662527) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n     </g>\n    </g>\n    <g id=\"line2d_36\">\n     <path d=\"M 485.7375 36.840652 \nL 495.7375 36.840652 \nL 505.7375 36.840652 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- train acc -->\n     <g transform=\"translate(513.7375 40.340652) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"325.830078\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"380.810547\"/>\n     </g>\n    </g>\n    <g id=\"line2d_37\">\n     <path d=\"M 485.7375 51.518777 \nL 495.7375 51.518777 \nL 505.7375 51.518777 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- validation loss -->\n     <g transform=\"translate(513.7375 55.018777) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"239.501953\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"300.78125\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"339.990234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"367.773438\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"428.955078\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"492.333984\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"524.121094\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"551.904297\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"613.085938\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"665.185547\"/>\n     </g>\n    </g>\n    <g id=\"line2d_38\">\n     <path d=\"M 485.7375 66.196902 \nL 495.7375 66.196902 \nL 505.7375 66.196902 \n\" style=\"fill: none; stroke-dasharray: 1.5,2.475; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_20\">\n     <!-- validation acc -->\n     <g transform=\"translate(513.7375 69.696902) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"239.501953\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"300.78125\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"339.990234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"367.773438\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"428.955078\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"492.333984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"524.121094\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"585.400391\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"640.380859\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p15b51f90a4\">\n   <rect x=\"36.465625\" y=\"9.06409\" width=\"558\" height=\"277.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: your answer here\n",
    "net_pos = TransformerNetwork(positional_encoding=True)\n",
    "train(net_pos, data_loaders, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How does the performance of a model with positional encoding compare to a model without?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T10:05:30.563127526Z",
     "start_time": "2023-10-06T10:05:30.542857430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 123+123=246\n",
      "  y_pred[0] tensor([[ -2.8903,  -3.4285,  -4.2426,  11.6620,  20.2189,  12.8258,  -3.5161,\n",
      "         -11.3862, -12.4141,  -5.9723,  -2.5251,  -1.5851,  -2.8720,  -0.4079,\n",
      "          -4.4101,  -2.7580],\n",
      "        [ -1.5241,   2.0898,   4.7034,  -1.6603,  -8.8719,   4.6135,  16.7391,\n",
      "          11.8273,  -1.3154, -14.9205, -15.2596,  -2.2820,  -1.1877,   3.1506,\n",
      "          -1.8310,  -1.7039],\n",
      "        [  0.9352,   2.2238,  -3.3709,  -8.3361, -12.6883,  -8.9845,  -3.3441,\n",
      "           5.9420,  16.4768,  10.4021,  -3.9104,  -3.7065,   1.2030,   3.0489,\n",
      "           1.0538,   0.9445],\n",
      "        [ -1.4169,   0.1853,  -2.5576,  -2.9432,  -4.7494,  -4.9949,  -1.7540,\n",
      "           0.4143,   3.7763,   1.6147,  -0.8177,  -0.6654,  -1.2557,  14.4985,\n",
      "          -2.0228,  -2.4246]], device='cuda:0')\n",
      "  encoded tensor([[ 4,  6,  8, 13]], device='cuda:0')\n",
      "  tokens [['2' '4' '6' '<eos>']]\n",
      "\n",
      "For 321+321=642\n",
      "  y_pred[0] tensor([[ -1.8517,   4.9649, -13.7908,  -8.0094,  -9.1677,  -5.6516,  -2.8389,\n",
      "           8.9223,  19.4812,  13.5285,  -2.5800,  -7.6889,  -1.9407,   0.0597,\n",
      "          -1.9835,  -2.4495],\n",
      "        [ -0.9821,   3.0719,  -1.2845,  -3.8544,  -7.0940,   8.7430,  19.0058,\n",
      "          11.6470,  -1.3561, -13.8296, -15.8413,  -5.2925,  -0.6531,   2.6203,\n",
      "          -1.5643,  -1.6150],\n",
      "        [ -0.9408,  -4.9110,   2.9746,   8.8947,  17.8856,  10.3812,  -0.7314,\n",
      "         -10.1078, -15.8855, -11.2929,  -4.7611,  -0.4445,  -1.1965,   1.9707,\n",
      "          -1.9369,  -0.6513],\n",
      "        [ -1.9502,  -0.4053,  -0.7217,   0.2083,   1.0621,   0.1153,  -0.1050,\n",
      "          -2.9551,  -3.3923,  -3.7249,  -2.0449,  -0.9046,  -2.0519,  14.8802,\n",
      "          -2.6966,  -3.2456]], device='cuda:0')\n",
      "  encoded tensor([[ 8,  6,  4, 13]], device='cuda:0')\n",
      "  tokens [['6' '4' '2' '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "for src, tgt in [('123+123', '246'), ('321+321', '642')]:\n",
    "    print(f'For {src}={tgt}')\n",
    "    y_pred = predict(net_pos, src, tgt)\n",
    "    print('  y_pred[0]', y_pred[0])\n",
    "    print('  encoded', torch.argmax(y_pred, dim=-1))\n",
    "    print('  tokens', decode_tokens(torch.argmax(y_pred, dim=-1)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compare the predictions for the first element of y with what you found earlier. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Explain in your own words why positional encoding is used in transformer networks.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Look at the learning curve. Can you suggest a way to improve the model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Optional: if time permits, try to train an even better model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Predicting for new samples (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting an output given a new sample requires an appropriate search algorithm (see [d2l chapter 10.8](https://d2l.ai/chapter_recurrent-modern/beam-search.html)). Here, we will implement the simplest form: a greedy search algorithm that selects the token with the highest probability at each time step.\n",
    "\n",
    "**(a) Describe this search strategy in pseudo-code.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Implement a greedy search function to predict a sequence using `net_pos`.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_greedy(net, src, length):\n",
    "    # predict an output sequence of the given (maximum) length given input string src\n",
    "    with torch.no_grad():\n",
    "        # TODO: Your code here.\n",
    "        pass\n",
    "\n",
    "\n",
    "predicted_sequence = predict_greedy(net_pos, '123+123', 6)\n",
    "print(decode_tokens(predicted_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Does this search strategy give a high-quality prediction? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) What alternative search strategy could we use to improve the predictions? Why would this help?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Discussion (4 points)\n",
    "\n",
    "Last week, we looked at recurrent neural networks such as the LSTM. Both recurrent neural networks and transformers work with sequences, but in recent years the transformer has become more popular than the recurrent models.\n",
    "\n",
    "**(a) An advantage of transformers over recurrent neural is that they can be faster to train. Why is that?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Does this advantage also hold when predicting outputs for new sequences? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Why is positional encoding often used in transformers, but not in convolutional or recurrent neural networks?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a recurrent neural network makes it very suitable for online predictions, such as real-time translation, because it only depends on prior inputs. You can design an architecture where the RNN produces an output token for every input token given to it, and it can produce that output without having to wait for the rest of the input.\n",
    "\n",
    "Note: 'online' means producing outputs continuously as new input comes in, as opposed to collecting a full dataset and analyzing it afterwards, it has nothing to do with the internet.\n",
    "\n",
    "**(d) How would a transformer work in an online application? Do you need to change the architecture?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 47 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 3c66915 / 2023-10-04</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
