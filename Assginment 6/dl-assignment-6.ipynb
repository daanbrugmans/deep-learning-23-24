{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment for week 6 of the 2022 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "\n",
    "**Group:**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Build a graph neural network, using pytorch geometric\n",
    "2. Compare a GNN with other network architectures\n",
    "3. Compare different GNN layers and aggregation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "As before you will need these libraries:\n",
    "* `torch`, `torch-sparse`, `torch-scatter`, and `torch-geometric` for PyTorch,\n",
    "* `d2l`, the library that comes with [Dive into deep learning](https://d2l.ai) book.\n",
    "\n",
    "The recommended way to install these libraries is described in the [torch-geometric installation instructions](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ${TORCH} and ${CUDA} with your torch and cuda versions.\n",
    "# Or remove the -f argument to compile from source\n",
    "#\n",
    "#!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline\n",
    "\n",
    "from d2l import torch as d2l\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch import nn\n",
    "from torch.nn import Linear, Dropout\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GraphConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 A node classification dataset (1 point)\n",
    "\n",
    "In this assignment we will be working on a node classification problem using the Citeseer dataset. This is a graph dataset that contains bag-of-words representation of documents and citation links between the documents. So there is an edge between document $i$ and document $j$ if one cites the other. This is an undirected edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data', name='Citeseer', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) How many graphs are there in this dataset? How large are they (in terms of nodes and edges)?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your answer here\n",
    "print(f'Number of graphs: {...}')...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we will continue the rest of this notebook using the first graph from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be use a subset of the nodes for training, and another subset for testing.\n",
    "These subsets are indicated by `data.train_mask` and `data.test_mask` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 MLP for node classification (6 points)\n",
    "\n",
    "In theory, we should be able to classify documents based only on their content, that is, using the bag-of-words features, without taking the graph structure into account.\n",
    "\n",
    "We can verify that by constructing a simple node-wise multilayer perceptron with a single hidden layer. This network does not use the edge information at all.\n",
    "\n",
    "**(a) Complete the code below.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "The network should have 2 linear layers. The hidden layer should have size `hidden_channels`, use ReLU activations, and use dropout with a dropout rate of 0.1. Don't use an activation function after the final layer.\n",
    "\n",
    "Hint: avoid using `Sequential`, it will make the assignment harder later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels = 16):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Complete the training loop below.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "Hint: compute the loss only on the training nodes.\n",
    "\n",
    "Hint 2: `data.x` contains the features for each node, `data.y` contains their labels.\n",
    "\n",
    "Hint 3: `model()` takes two parameters: a tensor of node features, and a tensor of edges. See the `test_accuracy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = d2l.try_gpu()\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    correct = pred_y.argmax(dim=1) == true_y\n",
    "    return int(correct.sum()) / len(true_y)\n",
    "\n",
    "def test(model, data):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        # Compute loss and accuracy only on the 'test' nodes\n",
    "        test_loss = loss_fn(out[data.test_mask], data.y[data.test_mask]).item()\n",
    "        test_acc = accuracy(out[data.test_mask], data.y[data.test_mask])\n",
    "        # Compute loss and accuracy only on the 'train' nodes\n",
    "        train_loss = loss_fn(out[data.train_mask], data.y[data.train_mask]).item()\n",
    "        train_acc = accuracy(out[data.train_mask], data.y[data.train_mask])\n",
    "        return train_loss, train_acc, test_loss, test_acc\n",
    "\n",
    "def train(model, data, lr=0.01, weight_decay=5e-4, epochs=400, plot=True):\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    if plot:\n",
    "        animator = d2l.Animator(xlabel='epoch', xlim=[1, epochs], figsize=(10, 5),\n",
    "                                legend=['train loss', 'train accuracy', 'test loss', 'test accuracy'])\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        # TODO: Compute and optimize loss\n",
    "        ...\n",
    "        \n",
    "        # Compute test accuracy, and plot\n",
    "        if plot and epoch % 10 == 0:\n",
    "            train_loss, train_acc, test_loss, test_acc = test(model, data)\n",
    "            animator.add(epoch + 1, (train_loss, train_acc, test_loss, test_acc))\n",
    "    \n",
    "    # Print final accuracy\n",
    "    train_loss, train_acc, test_loss, test_acc = test(model, data)\n",
    "    print(f'Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}')\n",
    "    print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Now construct and train an MLP on this dataset.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: construct and train the model\n",
    "mlp_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) The MLP network does not use the citation information at all. Give a way to incorporate the edge information without using a graph neural network?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note that the method should still work for arbitrary citation graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 A graph convolutional neural network (3 points)\n",
    "\n",
    "Next, we will use a graph neural network based on the Graph Convolutional Network approach, which was introduced in the paper [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Implement a graph convolutional neural network, by replacing the linear layers in the MLP with [`GCNConv` layers](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv), and train the network.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "The network should have two `GCNConv` layers. The rest of the architecture should stay as close as possible to the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels = 16):\n",
    "        super().__init__()\n",
    "        # TODO: initialize network layers\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # TODO: compute network output\n",
    "\n",
    "# TODO: construct and train the model\n",
    "gcn_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Compare the results of the MLP and the GCN. Which model is better?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Has the GCN training converged? Can you expect higher test accuracies by training longer? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Comparing GNN layers (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two graph layers that are interesting to compare are [`SAGEConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SAGEConv) and [`GraphConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv). Aside from one of them supporting weighted graphs, these models differ only in the accumulation function.\n",
    "\n",
    "**(a) Look at the documentation for these two layers. What is the difference in the accumulation function?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to copy the GNN structure every time, we can make our code generic in the type of layer to use.\n",
    "\n",
    "**(b) Make a generic graph neural network, that uses layers of type `layer_type`.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: you can construct layers with `my_layer = layer_type(in_size, out_size, **layer_args)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, layer_type, num_features, num_classes, hidden_channels=16, **layer_args):\n",
    "        super().__init__()\n",
    "        # TODO: initialize network layers\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # TODO: same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Train a SAGEConv network and a GraphConv network.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: construct and train a GNN with SAGEConv layers\n",
    "sageconv_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: construct and train a GNN with GraphConv layers\n",
    "graphconv_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Compare the performance of these two models, and also compare them to the GCN.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: look at the test loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Can you explain the observation in the previous question by looking at the aggregation functions? Why is one of them worse than the others?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, it is possible to use different aggregation functions, by passing `aggr=` to the network constructor.\n",
    "\n",
    "**(f) Compute the performance for `GraphConv` networks with `'mean'`, `'sum'`, `'min'`, `'max'`, and `'std'` aggregation.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: train with `plot=False` to only show the final loss and accuracy.\n",
    "\n",
    "Hint 2: if the performance is the same for all methods, there is most likely a bug in your `GNN` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your experiment here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Which three aggregation methods are the worst? For each one, explain why that one would not work well.<span style=\"float:right\"> (3 points)</span>**\n",
    "\n",
    "Hint: bag-of-word features are very sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Discussion (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Our training procedure gets the entire graph, including test nodes. Is it possible for the model to cheat using leaked information?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Can the GCN and GNN networks use information from neighbors of neighbors to classify a node? Briefly explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Do you think the trained model will generalize to other graphs? Motivate your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 21 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version f502e67 / 2023-10-04</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
