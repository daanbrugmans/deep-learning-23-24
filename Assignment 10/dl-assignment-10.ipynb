{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment for week 10 of the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "\n",
    "**Group:**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Build a variational autoencoder\n",
    "2. Extend the model to a conditional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "As before you will need these libraries:\n",
    "* `torch` and `torchvision` for PyTorch,\n",
    "* `d2l`, the library that comes with [Dive into deep learning](https://d2l.ai) book.\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline\n",
    "\n",
    "from d2l import torch as d2l\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 MNIST dataset (no points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will use the MNIST digit dataset. This dataset consists of 28×28 binary images and has 60000 training examples divided over 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Run the code below to load the MNIST dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {'batch_size':32, 'shuffle':True}\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    **opts)\n",
    "test_iter = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    **opts)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Variational Autoencoder (VAE) (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a Variational Autoencoder. This model consists of two networks: an encoder and a decoder.\n",
    "The encoder produces a distribution in the latent space, represented as the parameters of a normal distribution. The decoder takes the latent space representation and produces an output in the data space.\n",
    "\n",
    "**(a) Complete the implementation below.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size=2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Components of the encoder network\n",
    "        self.encoder_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*64, 16), nn.ReLU()\n",
    "        )\n",
    "        self.encoder_mean   = nn.Linear(16, latent_size)\n",
    "        self.encoder_logvar = nn.Linear(16, latent_size)\n",
    "        \n",
    "        # Components of the decoder\n",
    "        self.decoder_part1_z = nn.Linear(latent_size, 7*7*64)\n",
    "        self.decoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            # TODO: Choose an appropriate activation function for the final layer.\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder_part1(x)\n",
    "        return self.encoder_mean(h), self.encoder_logvar(h)\n",
    "\n",
    "    def sample_latent(self, mean_z, logvar_z):\n",
    "        eps = torch.randn_like(mean_z)\n",
    "        std_z = torch.exp(0.5 * logvar_z)\n",
    "        # TODO: turn the sample ε from N(0,1) into a sample from N(μ,σ)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_part1_z(z)\n",
    "        h = torch.reshape(h, (-1,64,7,7)) # Unflatten\n",
    "        return self.decoder_part2(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_z, logvar_z = self.encode(x)\n",
    "        z = self.sample_latent(mean_z, logvar_z)\n",
    "        return self.decode(z), mean_z, logvar_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some unit tests for the VAE\n",
    "samples = VAE().sample_latent(torch.ones(10000), torch.ones(10000))\n",
    "assert F.mse_loss(torch.mean(samples), torch.tensor(1.)) < 1e-3, \\\n",
    "      'sample_latent should produce values with the specified mean'\n",
    "assert F.mse_loss(torch.log(torch.var(samples)), torch.tensor(1.)) < 1e-3, \\\n",
    "      'sample_latent should produce values with the specified log variance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder produces two outputs that together give the parameters of a normal distribution: mean and logvar, so $\\mu$ and $\\log(\\sigma^2)$. The latter might seem strange, but there is a good reason for doing it this way. \n",
    "\n",
    "**(b) What can go wrong if the encoder network directly outputs mean and standard deviation (μ,σ)?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Loss function (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for a variational autoencoder consists of two parts:\n",
    "1. The reconstruction loss, which is the log likelihood of the data,\n",
    "$L_\\text{R} = \\log P(x\\mid z)$.\n",
    "2. The Kulback-Leibler divergence from the encoder output to the target distribution,\n",
    "$L_\\text{KL}= KL(Q(z)\\| P(z))$.\n",
    "\n",
    "In our case the data is binary, so we can use [binary cross entropy](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html) for the reconstruction loss.\n",
    "\n",
    "The derivation of the KL loss term can be found in appendix B of the VAE paper; [Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014](https://arxiv.org/pdf/1312.6114.pdf). Be careful:\n",
    "* the paper defines $-D_{KL}$, not $D_{KL}$\n",
    "* the sum is only over the latent space. In our code this corresponds to `axis=1`. Use the mean over the samples in the batch (`axis=0`).\n",
    "\n",
    "**(a) Implement the KL loss term below.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(recon_x, x):\n",
    "    # The reconstruction loss is binary cross entropy\n",
    "    # Note: we normalize the loss wrt. the batch size (len(x)), but not the size of the image\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum') / len(x)\n",
    "\n",
    "def kl_loss(mean_z, logvar_z):\n",
    "    # The KL divergence between a standard normal distribution and\n",
    "    #  a normal distribution with given mean and log-variance.\n",
    "    # TODO: your code here\n",
    "    return ...\n",
    "\n",
    "def loss_function(recon_x, x, mean_z, logvar_z):\n",
    "    l_recon = reconstruction_loss(recon_x, x)\n",
    "    l_kl    = kl_loss(mean_z, logvar_z)\n",
    "    return l_recon + l_kl, l_recon, l_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some unit tests for the loss function\n",
    "assert kl_loss(torch.tensor([[0]]), torch.tensor([[0]])) == 0, \\\n",
    "      'KL loss should be 0 for μ=0, σ=1'\n",
    "assert kl_loss(torch.tensor([[0]]), torch.tensor([[-1]])) > 0, \\\n",
    "      'KL loss should be > 0 for μ=0, σ<1'\n",
    "assert kl_loss(torch.tensor([[0]]), torch.tensor([[1]])) > 0, \\\n",
    "      'KL loss should be > 0 for μ=0, σ>1'\n",
    "assert kl_loss(torch.tensor([[1]]), torch.tensor([[0]])) > 0, \\\n",
    "      'KL loss should be > 0 for μ!=0, σ=1'\n",
    "assert kl_loss(torch.tensor([[0]]), torch.tensor([[1]])) == \\\n",
    "       kl_loss(torch.tensor([[0,0]]), torch.tensor([[1,1]])) / 2, \\\n",
    "      'Take the sum over the latent dimensions'\n",
    "assert kl_loss(torch.tensor([[0,0,1]]), torch.tensor([[0,1,-0.5]])) == \\\n",
    "       kl_loss(torch.tensor([[0,0,1],[0,0,1]]), torch.tensor([[0,1,-0.5],[0,1,-0.5]])), \\\n",
    "      'Take the mean over the items in the batch or normalize wrt. batch size (see also reconstruction_loss)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Training our VAE (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Complete the training loop below<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=10, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], figsize=(10, 5),\n",
    "                            legend=['train loss', 'train recon. loss', 'train KL loss',\n",
    "                                    'test loss', 'test recon. loss', 'test KL loss'])\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        model.train()\n",
    "        for x, y in train_iter:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # TODO: compute the outputs and loss\n",
    "            # TODO: backpropagate and apply optimizer\n",
    "            # Track our progress\n",
    "            metric.add(loss_recon.detach(), loss_kl.detach(), x.shape[0])\n",
    "        # Compute test loss\n",
    "        test_loss, test_loss_recon, test_loss_kl = test(model)\n",
    "        # Plot\n",
    "        train_loss_recon = metric[0] / metric[2]\n",
    "        train_loss_kl    = metric[1] / metric[2]\n",
    "        train_loss = train_loss_recon + train_loss_kl\n",
    "        animator.add(epoch + 1,\n",
    "                     (train_loss, train_loss_recon, train_loss_kl,\n",
    "                      test_loss, test_loss_recon, test_loss_kl))\n",
    "    print(f'training loss {train_loss:.3f}, test loss {test_loss:.3f}')\n",
    "    print(f'training reconstruction loss {train_loss_recon:.3f}, test reconstruction loss {test_loss_recon:.3f}')\n",
    "    print(f'training KL loss {train_loss_kl:.3f}, test KL loss {test_loss_kl:.3f}')\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    metric = d2l.Accumulator(3)\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(test_iter):\n",
    "            x = x.to(device)\n",
    "            # TODO: compute the outputs and loss\n",
    "            metric.add(loss_recon, loss_kl, x.shape[0])\n",
    "    test_loss_recon = metric[0] / metric[2]\n",
    "    test_loss_kl    = metric[1] / metric[2]\n",
    "    return test_loss_recon + test_loss_kl, test_loss_recon, test_loss_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Train the model.<span style=\"float:right\"> (no points)</span>**\n",
    "\n",
    "Hint: the training and test loss should both be around 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = d2l.try_gpu()\n",
    "model = VAE().to(device)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) If you increase the number of latent dimensions, how does that affect the reconstruction loss and the KL loss terms?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Visualizing the latent space (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function below to visualize the 2D latent space, by running the decoder on $z$ values sampled at regular intervals.\n",
    "\n",
    "**(a) Complete the code below and run it to plot the latent space.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(model):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    n = 31\n",
    "    digit_size = 28\n",
    "    scale = 2.0\n",
    "    figsize = 10\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            # TODO: run the decoder on z = [xi,yi].\n",
    "            x_decoded = ...\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = x_decoded.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Would it be possible to classify digits based on this latent representation? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) If you retrain the model, would you expect the latent space to look exactly the same. If not, what differences can you expect?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of visualizing the latent space is by making a scatter plot of the training data in the latent space.\n",
    "\n",
    "**(d) Complete and run the code below to make a scatterplot of the training data.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_latent(model):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    zs, ys = [], []\n",
    "    for x, y in itertools.islice(train_iter, 100):\n",
    "        # TODO: compute mean z\n",
    "        z_mean = ...\n",
    "        zs.append(z_mean.detach().cpu())\n",
    "        ys.append(y)\n",
    "    zs = torch.cat(zs).numpy()\n",
    "    ys = torch.cat(ys).numpy()\n",
    "    \n",
    "    cmap = plt.get_cmap('jet', 10)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(zs[:, 0], zs[:, 1], c=ys, cmap=cmap, alpha=0.8, vmin=-0.5, vmax=9.5)\n",
    "    plt.colorbar(ticks=np.arange(0, 10))\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "scatterplot_latent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compare this figure to the one from `plot_latent`. How are the plots related?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: Don't just answer \"both visualize the latent space\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Compared to the figure from `plot_latent`, what information about the VAE is shown in this figure but not in the previous one?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) What distribution should we expect the points in the latent space to follow, based on the KL divergence term in the loss function?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Look at the distribution of the data in the latent space. Does the plot match the answer to the previous question? If not, why?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Conditional Variational Autoencoder (10 points)\n",
    "\n",
    "An extension of variational autoencoders uses labels to *condition* the encoder and decoder models.\n",
    "In this *conditional VAE*, the decoder becomes $P(x|z,y)$ and the encoder $Q(z|x,y)$.\n",
    "In practice, this means that the label $y$ is given as an extra input to the both the encoder and the decoder.\n",
    "\n",
    "For details see the paper [Semi-Supervised Learning with Deep Generative Models; Kingma, Rezende, Mohamed, Welling; 2014](https://arxiv.org/pdf/1406.5298.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the labels in the decoder, we can concatenate the label with the latent vector. Or equivalently, we can use separate weights for $z$ and $y$ in the first layer, so that layer computes $W_z \\cdot z + W_y \\cdot y + b$.\n",
    "\n",
    "Similarly for the encoder, except there we will still use a convolutional layer for $x$, combined with a fully connected layer for $y$.\n",
    "\n",
    "**(a) Complete the implementation of the conditional VAE below.<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, latent_size=2, num_classes=10):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Components of the encoder network\n",
    "        # TODO: split the first layer from the previous encoder network into a separate variable,\n",
    "        #       and add a layer to use with the y input\n",
    "        self.encoder_part1_x = ...\n",
    "        self.encoder_part1_y = ...\n",
    "        self.encoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*64, 16), nn.ReLU()\n",
    "        )\n",
    "        self.encoder_mean   = nn.Linear(16, latent_size)\n",
    "        self.encoder_logvar = nn.Linear(16, latent_size)\n",
    "\n",
    "        # Components of the decoder network\n",
    "        self.decoder_part1_z = nn.Linear(latent_size, 7*7*64)\n",
    "        # TODO: add layer to use with the y input\n",
    "        self.decoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            # TODO: see VAE\n",
    "        )\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        h = self.encoder_part1_x(x) + self.encoder_part1_y(y).reshape(-1,32,14,14)\n",
    "        h = self.encoder_part2(h)\n",
    "        return self.encoder_mean(h), self.encoder_logvar(h)\n",
    "\n",
    "    def sample_latent(self, mean_z, logvar_z):\n",
    "        eps = torch.randn_like(mean_z)\n",
    "        std_z = torch.exp(0.5 * logvar_z)\n",
    "        # TODO: see VAE\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        # TODO: use a first layer that combines z and y\n",
    "        h = ...\n",
    "        h = torch.reshape(h, (-1,64,7,7))\n",
    "        return self.decoder_part2(h)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mean_z, logvar_z = self.encode(x, y)\n",
    "        z = self.sample_latent(mean_z, logvar_z)\n",
    "        return self.decode(z, y), mean_z, logvar_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Copy the training code from section 10.4, and modify it for a conditional VAE.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: To train the conditional VAE we need to use one-hot encoding of the labels. You can use the following code for that:\n",
    "\n",
    "    y = F.one_hot(y,10).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cvae(model, num_epochs=10, lr=1e-3):\n",
    "    # TODO: your code here\n",
    "\n",
    "def test_cvae(model):\n",
    "    # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Train a conditional VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_model = ConditionalVAE().to(device)\n",
    "train_cvae(cvae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Adapt the `plot_latent` function from section 10.5 for conditional VAEs, and use your function to visualize the latent space for the classes `4` and `8`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_cvae(...):    # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) What do the latent dimensions represent? Is this the same for all labels?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Adapt `scatterplot_latent` to show the distribution in the latent space.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_latent_cvae(cvae_model):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "\n",
    "scatterplot_latent_cvae(cvae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) How is this distribution in the latent space different from the distribution of the VAE? Compare to your answer to that for question 10.5 g and h. What is the cause of these differences?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Would it be possible to classify digits based on the latent representation of the conditional VAE? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) Describe how you could use a conditional VAE to change the label or content of an image, while keeping the style as similar as possible.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Discussion (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Is the conditional VAE a strict improvement over the normal VAE in all cases?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Compare the latent representation vector $z$ in the VAE with the input for the generator in a GAN. They are both small vectors, and they are both often called $z$. In what way are they the same, and in what way are the different?<span class=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 28 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 7536f1e / 2023-11-15</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
